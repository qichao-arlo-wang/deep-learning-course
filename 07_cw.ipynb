{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb9t6NTe-6BL"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/07_VAE_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcG_R3OAGe5m"
      },
      "source": [
        "# Coursework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpQekp8aNVt_"
      },
      "source": [
        "\n",
        "## Task 1: MNIST generation using VAE and GAN\n",
        "\n",
        "**Report**\n",
        "* Train the given VAE model in the tutorial also using the same hyper parameters (batch size, optimizer, number of epochs, etc...) but increasing the latent dimensionality to 10. Compute the MSE on the reconstructed `x_test` images and Inception Score (IS). Now train the same model without the KL divergence loss, and compute again the MSE and IS score. Report the results in a table and discuss them. For the IS you can use in this case `compute_inception_score(model, 10, denormalize=False)`.\n",
        "\n",
        "* Train the GAN given in the tutorial with increased dimensionality of the initial random sample to 10 for 10 epochs and report its IS (use the same table as in the VAE case). Discuss the difference in the obtained IS for VAE and GAN and link it to the qualitative results. For the IS use in this case `compute_inception_score(model, 10, denormalize=True)`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKkjwCF6uw3K"
      },
      "source": [
        "## Task 2: Quantitative VS Qualitative Results\n",
        "\n",
        "In this task, we will observe the difference between two trained models for colouring images. One is the model trained during the tutorial, which uses a cGAN approach to predict the RGB pixel-wise values of a B&W image. The other one is a simple UNet autoencoder trained with a Mean Absolute Error (MAE) loss, which is trained to predict directly the RBG image without any GAN based learning strategy. We refer to the first and second models as cGAN and MAE models, respectively. For this task, 20 epochs trained weights for the cGAN and MAE models are provided. If desired, the code to train the MAE model can be found below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTojDadnariK"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Training Hyperparameters\n",
        "img_shape = (32, 32)\n",
        "num_epochs = 20\n",
        "batch_size = 128\n",
        "lr = 2.0e-4\n",
        "betas = (0.5, 0.999)\n",
        "\n",
        "# Data\n",
        "train_dataset = Cifar('CIFAR10', train=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Model, optimizer and criterion (MAE)\n",
        "generator_mae = CGenerator().to(DEVICE)\n",
        "optimizer_mae = torch.optim.Adam(generator_mae.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "criterion_mae = nn.L1Loss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    g_avg_loss = []\n",
        "\n",
        "    for batch_i, (imgs_rgb, imgs_bw) in enumerate(train_loader):\n",
        "        # Move data to device\n",
        "        imgs_rgb, imgs_bw = imgs_rgb.to(DEVICE), imgs_bw.to(DEVICE)\n",
        "\n",
        "        # Generate fake rgb images\n",
        "        optimizer_mae.zero_grad()\n",
        "        imgs_rgb_generated = generator_mae(imgs_bw)\n",
        "        g_loss = criterion_mae(imgs_rgb_generated, imgs_rgb)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        g_loss.backward()\n",
        "        optimizer_mae.step()\n",
        "\n",
        "        g_avg_loss.append(g_loss.item())\n",
        "\n",
        "        # Plot examples\n",
        "        if batch_i % 50 == 0:\n",
        "            with torch.no_grad():\n",
        "                show_colored_images(imgs_rgb, imgs_bw, imgs_rgb_generated)\n",
        "\n",
        "        # Print progress\n",
        "        if batch_i % 10 == 0:\n",
        "            print(f\"[Epoch {epoch}/{num_epochs}] [Batch {batch_i}/{len(train_loader)}] \"\n",
        "                  f\"[G loss: {np.mean(g_avg_loss):.4f}] \")\n",
        "\n",
        "# Save model\n",
        "torch.save(generator_mae.state_dict(), 'generator_mae.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9aTGM7qwtyO"
      },
      "source": [
        "Instead of training the models, we can directly load their pre-trained weights by running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNkt5HIHBM6_"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/MatchLab-Imperial/deep-learning-course/raw/master/asset/07_VAE_GAN/cgan_cifar10_epoch20.pth\n",
        "!wget https://github.com/MatchLab-Imperial/deep-learning-course/raw/master/asset/07_VAE_GAN/cgenerator_mae_cifar10_epoch20.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w71OzOuaMmg"
      },
      "outputs": [],
      "source": [
        "# Pre-trained CGAN\n",
        "generator_cGAN = CGAN().to(DEVICE)\n",
        "generator_cGAN.load_state_dict(torch.load('cgan_cifar10_epoch20.pth'))\n",
        "generator_cGAN.eval()\n",
        "\n",
        "# Pre-trained MAE model\n",
        "generator_mae = CGenerator().to(DEVICE)\n",
        "generator_mae.load_state_dict(torch.load('cgenerator_mae_cifar10_epoch20.pth'))\n",
        "generator_mae.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyXI9hCTzUn2"
      },
      "source": [
        "We have loaded both models, and we are ready to compare them. In this task, you are asked to analyse the difference between the quantitative versus the qualitative results. To do so, we provided two pieces of code. The first one will compute the MAE metric for both models in the test dataset. As we know, this metric is widely used on image generation tasks, such as image upsampling, image reconstruction, image translation, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t46neFcx6nyg"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "test_dataset = Cifar('CIFAR10', train=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Loss\n",
        "l1_mae, l1_cgan = [], []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for imgs_rgb, imgs_bw in test_loader:\n",
        "        imgs_rgb, imgs_bw = imgs_rgb.to(DEVICE), imgs_bw.to(DEVICE)\n",
        "        imgs_rgb_generated_mae = generator_mae(imgs_bw)\n",
        "        imgs_rgb_generated_cgan = generator_cGAN(imgs_bw)\n",
        "\n",
        "        l1_mae.append(F.l1_loss(imgs_rgb_generated_mae, imgs_rgb).item())\n",
        "        l1_cgan.append(F.l1_loss(imgs_rgb_generated_cgan, imgs_rgb).item())\n",
        "\n",
        "print(\"MAE (Trained MAE): {:.4f}\".format(np.mean(l1_mae)))\n",
        "print(\"MAE (Trained cGAN): {:.4f}\".format(np.mean(l1_cgan)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddZgs-Il8F9B"
      },
      "source": [
        "The next piece of code will show coloured examples for both networks, so you can check them visually and discuss which model is better. First, we need to create an iterator object to go through the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuPRF6DZeHyR"
      },
      "outputs": [],
      "source": [
        "# Change seed and num_samples here to see different samples\n",
        "set_seed(42)\n",
        "num_samples = 3\n",
        "\n",
        "# Data\n",
        "test_dataset = Cifar('CIFAR10', train=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    img_rgb, img_bw = next(iter(test_loader))\n",
        "    img_rgb, img_bw = img_rgb.to(DEVICE), img_bw.to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_rgb_generated_mae = generator_mae(img_bw)\n",
        "        img_rgb_generated_cgan = generator_cGAN(img_bw)\n",
        "        show_colored_two_models(img_rgb, img_bw, img_rgb_generated_mae, img_rgb_generated_cgan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss_1bTVE-Us_"
      },
      "source": [
        "We showed that both models obtain a similar MAE value. If we would only take into account the quantitative metric, as done in many scientific articles, we would say that the MAE model is better. However, in addition to the quantitative results, we need to analyse visually the results produced by the two networks to declare which is the best model.\n",
        "\n",
        "**Report**\n",
        "\n",
        "\n",
        "*   Run the previous code to analyse several coloured images for both models. Based on previous results and linked to GAN theory, discuss from the numerical and visual perspective if both models are similar, or whether there is a better one. You can provide in the report visual examples together with their MAE values to support your arguments. The figure of this task can be included in the Appendix. Discussion still needs to go into the main text."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
