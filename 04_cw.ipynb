{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/04_Common_CNN_architectures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsKc0btBtDyy"
      },
      "source": [
        "# **Coursework**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urw5Sugu1F7r"
      },
      "source": [
        "## Task 1: Classification on Tiny-ImageNet\n",
        "\n",
        "In this task, we are going to explore different models to do classification on 64x64 Tiny-ImageNet. Tiny-ImageNet is a smaller version of ImageNet (as the name indicates), containing \"only\" 200 classes. Each class has 500 images. The test set contains 10,000 images. All images are 64x64 RGB images.\n",
        "\n",
        "In the Network Training notebook, we explained how to define a validation set, and now we will put that into practice. Hence, as we now have a bigger dataset, we are going to use the standard split of training, validation, and test data. Therefore, you will check the performance of the network in the validation set while training your network. Hence, your decisions need to be based on validation performance. Once you have obtained your best model using the training and validation data, you need to report the performance on the test set. Please try to no overfit to the test data, as in other problems it may not be available to you.\n",
        "\n",
        "In this exercise, you are asked to train VGG models with different strategies. Optionally, you are asked to use any other architecture of your choice to do classification in Tiny-ImageNet.\n",
        "\n",
        "Run the following script to get the data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOgLmEaJmCD4"
      },
      "outputs": [],
      "source": [
        "# Download TinyImageNet\n",
        "! git clone https://github.com/seshuad/IMagenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmPJ2vKG5BB_"
      },
      "outputs": [],
      "source": [
        "def get_id_dictionary():\n",
        "    id_dict = {}\n",
        "    for i, line in enumerate(open('IMagenet/tiny-imagenet-200/wnids.txt', 'r')):\n",
        "        id_dict[line.replace('\\n', '')] = i\n",
        "    return id_dict\n",
        "\n",
        "def get_class_to_id_dict():\n",
        "    id_dict = get_id_dictionary()\n",
        "    all_classes = {}\n",
        "    result = {}\n",
        "    for i, line in enumerate(open('IMagenet/tiny-imagenet-200/words.txt', 'r')):\n",
        "        n_id, word = line.split('\\t')[:2]\n",
        "        all_classes[n_id] = word\n",
        "    for key, value in id_dict.items():\n",
        "        result[value] = (key, all_classes[key])\n",
        "\n",
        "    return result\n",
        "\n",
        "def get_data(id_dict):\n",
        "    train_data, val_data, test_data = [], [], []\n",
        "    train_labels, val_labels, test_labels = [], [], []\n",
        "    for key, value in id_dict.items():\n",
        "        train_data += [cv2.imread('IMagenet/tiny-imagenet-200/train/{}/images/{}_{}.JPEG'.format(key, key, str(i))) for i in range(450)]\n",
        "        train_labels += [value] * 450\n",
        "\n",
        "        val_data += [cv2.imread('IMagenet/tiny-imagenet-200/train/{}/images/{}_{}.JPEG'.format(key, key, str(i))) for i in range(450, 500)]\n",
        "        val_labels += [value] * 50\n",
        "\n",
        "    for line in open('IMagenet/tiny-imagenet-200/val/val_annotations.txt'):\n",
        "        img_name, class_id = line.split('\\t')[:2]\n",
        "        test_data.append(cv2.imread(f'IMagenet/tiny-imagenet-200/val/images/{img_name}'))\n",
        "        test_labels.append(id_dict[class_id])\n",
        "\n",
        "    return np.array(train_data), np.array(train_labels), np.array(val_data), np.array(val_labels), np.array(test_data), np.array(test_labels)\n",
        "\n",
        "def shuffle_data(train_data, train_labels, val_data, val_labels):\n",
        "    # This function shuffles separately the train set and the\n",
        "    # validation set\n",
        "    size = len(train_data)\n",
        "    train_idx = np.arange(size)\n",
        "    np.random.shuffle(train_idx)\n",
        "\n",
        "    size = len(val_data)\n",
        "    val_idx = np.arange(size)\n",
        "    np.random.shuffle(val_idx)\n",
        "\n",
        "    return train_data[train_idx], train_labels[train_idx], val_data[val_idx], val_labels[val_idx]\n",
        "\n",
        "train_data, train_labels, val_data, val_labels, test_data, test_labels = get_data(get_id_dictionary())\n",
        "train_data, train_labels, val_data, val_labels = shuffle_data(train_data, train_labels, val_data, val_labels)\n",
        "\n",
        "# Let's visualize some examples\n",
        "N=3\n",
        "start_val = 0 # pick an element for the code to plot the following N**2 values\n",
        "fig, axes = plt.subplots(N,N)\n",
        "for row in range(N):\n",
        "  for col in range(N):\n",
        "    idx = start_val+row+N*col\n",
        "    tmp = cv2.cvtColor(train_data[idx],cv2.COLOR_BGR2RGB)\n",
        "    axes[row,col].imshow(tmp, cmap='gray')\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    axes[row,col].set_xticks([])\n",
        "    axes[row,col].set_yticks([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzcWKswmmRJP"
      },
      "outputs": [],
      "source": [
        "train_data, train_labels, val_data, val_labels, test_data, test_labels = get_data(get_id_dictionary())\n",
        "train_data, train_labels, val_data, val_labels = shuffle_data(train_data, train_labels, val_data, val_labels)\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "train_data = torch.from_numpy(train_data).permute(0, 3, 1, 2).float() / 255.0\n",
        "val_data   = torch.from_numpy(val_data).permute(0, 3, 1, 2).float() / 255.0\n",
        "test_data  = torch.from_numpy(test_data).permute(0, 3, 1, 2).float() / 255.0\n",
        "\n",
        "# Normalize to [-1, 1], channel independent\n",
        "mean = train_data.mean(dim=(0, 2, 3))\n",
        "std  = train_data.std(dim=(0, 2, 3))\n",
        "train_data = (train_data - mean[None, :, None, None]) / (std[None, :, None, None] + 1e-7)\n",
        "val_data   = (val_data   - mean[None, :, None, None]) / (std[None, :, None, None] + 1e-7)\n",
        "test_data  = (test_data  - mean[None, :, None, None]) / (std[None, :, None, None] + 1e-7)\n",
        "\n",
        "# Build data loader\n",
        "train_dataset = TensorDataset(train_data, torch.from_numpy(train_labels))\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(val_data, torch.from_numpy(val_labels))\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "test_dataset = TensorDataset(test_data, torch.from_numpy(test_labels))\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXLjtvJEAF_r"
      },
      "source": [
        "**Report**:\n",
        "*   In a plot, please report the training and validation accuracy curves for the following models:\n",
        "\n",
        "> *   VGG16 trained from scratch.\n",
        "\n",
        "> *   Transfer Learning VGG16: load pre-trained ImageNet weights and only train the newly added dense layers.  To do so, freeze all layers and only train the dense layers you have modified in the model.\n",
        "\n",
        "> *   Fine-tuning VGG16: load pre-trained ImageNet weights and train the whole architecture.\n",
        "\n",
        "*   Discuss the previous figure in the main text. And report in a table the test accuracy and the training and inference times of previous VGG16 experiments. Training times are computed per epoch and you can find them displayed in the .fit() method information. Report either the total training time, or the number of epochs and training time per epoch. Inference times are computed per image, and we give you the code below to obtain them.\n",
        "\n",
        "*   Now that we are familiar with loading and using models in Pytorch, you can use any model of your choice to classify Tiny-ImageNet. You can take the model directly from Pytorch, any GitHub repository, or do the code yourself. You need to report your results in the previous table and compare your model of choice with previous VGG16 networks.\n",
        "\n",
        "Note that training/inference time will depend on which GPU you are using. Report the time results in the same instance, or at least when using the same GPU. Report also the GPU you were using to compute those inference times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae4MXFHF6Lsk"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "# You may want to import more modules.\n",
        "\n",
        "# Early stopping utility\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_accuracy = None\n",
        "        self.best_model_weight = None\n",
        "\n",
        "    def __call__(self, model, val_accuracy):\n",
        "        if self.best_accuracy is None:\n",
        "            self.best_accuracy = val_accuracy\n",
        "            self.best_model_weight = copy.deepcopy(model.state_dict())\n",
        "        elif val_accuracy < self.best_accuracy + self.min_delta:\n",
        "            self.counter += 1\n",
        "            return self.counter >= self.patience\n",
        "        else:\n",
        "            self.best_accuracy = val_accuracy\n",
        "            self.best_model_weight = copy.deepcopy(model.state_dict())\n",
        "            self.counter = 0\n",
        "            return False\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Define your model here\n",
        "# model = ...\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "early_stopping = EarlyStopping(patience=5, min_delta=1e-3)\n",
        "\n",
        "for epoch in range(20):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0.0\n",
        "\n",
        "    time_start = time.time()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    time_end = time.time()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_accuracy = train_correct / len(train_loader.dataset) * 100\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accuracy = val_correct / len(val_loader.dataset) * 100\n",
        "\n",
        "    print((\n",
        "        f\"Epoch [{epoch+1}/20] \"\n",
        "        f\"Train Loss: {train_loss:.4f}, \"\n",
        "        f\"Train Accuracy: {train_accuracy:.2f}%, \"\n",
        "        f\"Validation Loss: {val_loss:.4f}, \"\n",
        "        f\"Validation Accuracy: {val_accuracy:.2f}%, \"\n",
        "        f\"Training Time/Epoch: {time_end-time_start:.2f}s\"\n",
        "    ))\n",
        "\n",
        "    # Early stopping check\n",
        "    if early_stopping(model, val_accuracy):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        print()\n",
        "        break\n",
        "\n",
        "# Load best model weights\n",
        "model.load_state_dict(early_stopping.best_model_weight)\n",
        "\n",
        "# Evaluate on test set\n",
        "running_time = 0.0\n",
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        time_start = time.time()\n",
        "        outputs = model(inputs)\n",
        "        time_end = time.time()\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "        test_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "        running_time += time_end - time_start\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = test_correct / len(test_loader.dataset) * 100\n",
        "time_per_image = running_time / len(test_loader.dataset)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Inference Time/Image: {time_per_image:.4f}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKlWOwzps6b7"
      },
      "source": [
        "## Task 2: ConvNeXt Model Scaling on Tiny-ImageNet\n",
        "ConvNeXt is a modern convolutional neural network architecture introduced by Facebook AI Research in the 2022 paper titled \"[A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)\" by Liu et al. It reimagines the classic ResNet architecture by integrating design principles from Vision Transformers (ViTs), such as large kernel sizes, inverted bottlenecks, depthwise convolutions, Layer Normalization, and GELU activations. Despite being fully convolutional, ConvNeXt matches or even surpasses the performance of transformer-based models like the Swin Transformer on benchmarks such as ImageNet. Its success demonstrated that, with the right architectural updates, CNNs can remain competitive in the transformer era, significantly influencing how researchers view the future of convolutional models in computer vision.\n",
        "\n",
        "Pytorch offers pretrained model architecture for ConvNeXt at varying sizes, which can be seen [here](https://docs.pytorch.org/vision/main/models/convnext.html).\n",
        "\n",
        "In this task, we explore how increasing model size affects performance while keeping architecture the same. You will load ConvNeXt-Tiny, Small, Base, and Large pretrained on ImageNet, and evaluate them on the previously loaded TinyImageNet data to investigate the tradeoff in performance for size constraints.\n",
        "\n",
        "*Note: You are NOT training or fine-tuning these models.*\n",
        "\n",
        "Use the code below to load the data in the format required by the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t1nC0dDwH81"
      },
      "outputs": [],
      "source": [
        "# Define transforms for ConvNeXt input (resize to 224, normalize like ImageNet)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class TinyImageNetTestDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = np.argmax(labels, axis=1)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "test_dataset = TinyImageNetTestDataset(test_data, test_labels, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7jEtQ8hzpZC"
      },
      "outputs": [],
      "source": [
        "def fit_model_head(model, num_classes=200):\n",
        "    # Map model_name string to torchvision convnext constructor\n",
        "    # Replace the classifier head to match Tiny-ImageNet classes\n",
        "    num_ftrs = model.classifier[2].in_features  # ConvNeXt classifier is nn.Sequential(..., nn.Linear)\n",
        "    model.classifier[2] = nn.Linear(num_ftrs, num_classes)\n",
        "    return model\n",
        "\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhEarez0zpuW"
      },
      "source": [
        "**Report**:\n",
        "*   In a plot, report the accuracy and test MSE against number of parameters of the model.\n",
        "\n",
        "*   Discuss the previous figure(s) in the main text, explaining the trend seen between model size and performance and comment on the benefits/drawbacks of using a larger/smaller model.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
