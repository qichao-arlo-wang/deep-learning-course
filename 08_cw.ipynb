{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/08_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWgV2gkTfpCc"
      },
      "source": [
        "# Coursework\n",
        "\n",
        "## Task 1: On-policy vs. Off-policy\n",
        "Use the code given below to run the training loop, where the agent is trained for 200 episodes. The agent we give follows a Q-learning approach, which is an off-policy approach. You will now change the approach to SARSA, which is an on-policy approach. Also, for both Q-learning and SARSA test two different policies: $\\epsilon$-greedy and Softmax. $\\epsilon$-greedy is already defined in the tutorial and implemented in the given agent. Softmax policy refers to sampling the next action following the probability distribution given by $Softmax(Q(s, a))$. We provide you the NumPy softmax function to normalize the Q-Values into a probability function to use before sampling. Similarly to RNN, in the softmax function, there is a temperature value involved, we set a default value that works, but you can tweak it if you find another value with better performance. Report the new value if you decide to do so.\n",
        "\n",
        "You will need to modify `act` and `replay` from the `DQNAgent` to implement the different approaches we ask for. Results may differ from run to run due to different initialization states.\n",
        "\n",
        "**Report**\n",
        "* Plot the average reward for the last 50 episodes vs. number of training episodes (train for 200 episodes) for the four agents trained: Q-learning and SARSA with both $\\epsilon$-greedy policy and Softmax policy.\n",
        "\n",
        "* Attach in the Appendix the modifications done to `DQNAgent` to implement the different agents. Do not include your code, a simple explanation with the key modifications is enough.\n",
        "\n",
        "* In addition to the average reward plot, include **at least one secondary plot** that illustrates the behavioral differences between the four agents. For example, you might consider:\n",
        "\n",
        "  ---\n",
        "\n",
        "  **$\\epsilon$-greedy vs. Softmax**\n",
        "\n",
        "  * **Fraction of greedy actions**  \n",
        "  Proportion of times in an episode the agent chooses the action with the highest estimated Q-value.  \n",
        "  Shows how quickly exploration gives way to exploitation, and is useful when comparing ϵ-greedy and Softmax.  \n",
        "\n",
        "  * **Average policy entropy**  \n",
        "  A measure of how “spread out” the action probabilities are per-step in an episode (compute an episode average).  \n",
        "  High entropy means the agent is exploring more uniformly; low entropy means it is being more deterministic.  \n",
        "\n",
        "  ---\n",
        "\n",
        "  **Q-Learning vs. SARSA**\n",
        "\n",
        "  * **Variance of returns**  \n",
        "  Plot a rolling variance across episodes of total episode reward (e.g., window = 20).  \n",
        "  Highlights stability differences between Q-Learning and SARSA.  \n",
        "\n",
        "  * **Average Temporal-difference (TD) error magnitude**  \n",
        "  Measures how different the agent’s current Q-value estimate is from the updated target it just computed.  \n",
        "  Captures how big the agent’s “surprise” is at each step.  \n",
        "\n",
        "    - For Q-learning (off-policy):  \n",
        "      $\n",
        "      \\delta = \\left| r + \\gamma \\max_{a} Q(s', a) - Q(s, a) \\right|\n",
        "      $\n",
        "\n",
        "    - For SARSA (on-policy):  \n",
        "      $\n",
        "      \\delta = \\left| r + \\gamma Q(s', a') - Q(s, a) \\right|, \\quad $where  $a'$ is the next action chosen by the policy.\n",
        "\n",
        "\n",
        "    Large TD errors mean the agent is still making big adjustments (its predictions are far off).  \n",
        "    Smaller TD errors suggest the estimates are stabilizing.  \n",
        "\n",
        "  ---\n",
        "\n",
        "  These are suggestions — you are welcome to choose other metrics if you find them insightful, as long as you clearly explain how your plot highlights differences between the agents.\n",
        "  \n",
        "  To make trends easier to see, consider smoothing your plots with a **moving average** across episodes (e.g. a 20-episode window).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A47sXwdmhlrU"
      },
      "outputs": [],
      "source": [
        "def softmax(x, temperature=0.025):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    x = (x - np.expand_dims(np.max(x, 1), 1))\n",
        "    x = x/temperature\n",
        "    e_x = np.exp(x)\n",
        "    return e_x / (np.expand_dims(e_x.sum(1), -1) + 1e-5)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, device=None):\n",
        "        self.state_size   = state_size\n",
        "        self.action_size  = action_size\n",
        "        self.memory       = collections.deque(maxlen=20000)\n",
        "        self.gamma        = 0.95\n",
        "        self.epsilon      = 1.0\n",
        "        self.epsilon_min  = 0.01\n",
        "        self.epsilon_decay= 0.995\n",
        "        self.learning_rate= 0.001\n",
        "\n",
        "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model  = self._build_model().to(self.device)\n",
        "        self.opt    = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.loss_fn= nn.MSELoss()\n",
        "\n",
        "    def _build_model(self):\n",
        "        net = nn.Sequential(\n",
        "            nn.Linear(self.state_size, 24),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(24, 48),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(48, self.action_size)  # Q(s,·)\n",
        "        )\n",
        "        return net\n",
        "\n",
        "    # ---- replay buffer ----\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    # ---- ε-greedy & exploit ----\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            s = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
        "            q = self.model(s)  # (1, A)\n",
        "            return int(torch.argmax(q, dim=1).item())\n",
        "\n",
        "    def exploit(self, state):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            s = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
        "            q = self.model(s)\n",
        "            return int(torch.argmax(q, dim=1).item())\n",
        "\n",
        "    # ---- Q-learning update via replay ----\n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        state_b      = torch.as_tensor(np.vstack([m[0] for m in minibatch]),\n",
        "                                       dtype=torch.float32, device=self.device)\n",
        "        action_b     = torch.as_tensor([m[1] for m in minibatch],\n",
        "                                       dtype=torch.long, device=self.device)\n",
        "        reward_b     = torch.as_tensor([m[2] for m in minibatch]),\n",
        "        reward_b     = reward_b[0].to(self.device).float()\n",
        "        next_state_b = torch.as_tensor(np.vstack([m[3] for m in minibatch]),\n",
        "                                       dtype=torch.float32, device=self.device)\n",
        "        done_b       = torch.as_tensor([m[4] for m in minibatch],\n",
        "                                       dtype=torch.float32, device=self.device)\n",
        "\n",
        "        # target = r + γ * max_a' Q(next_state, a'); if done -> r\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            next_q_max = self.model(next_state_b).max(dim=1).values\n",
        "            target_scalar = reward_b + self.gamma * next_q_max * (1.0 - done_b)\n",
        "\n",
        "            # target_f: start from current Q(s,·), overwrite chosen action with target_scalar\n",
        "            target_full = self.model(state_b).clone()\n",
        "            target_full[torch.arange(batch_size, device=self.device), action_b] = target_scalar\n",
        "\n",
        "        # fit to target_full\n",
        "        self.model.train()\n",
        "        pred_q = self.model(state_b)\n",
        "        loss = self.loss_fn(pred_q, target_full)\n",
        "\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "        self.opt.step()\n",
        "\n",
        "        # ε decay\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    # ---- save/load weights ----\n",
        "    def save(self, path):\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        state = torch.load(path, map_location=self.device)\n",
        "        self.model.load_state_dict(state)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiW8Mr6VfvgM"
      },
      "outputs": [],
      "source": [
        "EPISODES = 200\n",
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "batch_size = 32\n",
        "episode_reward_list = collections.deque(maxlen=50)\n",
        "\n",
        "for e in range(EPISODES):\n",
        "    state, _ = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    total_reward = 0\n",
        "    for time in range(200):\n",
        "      action = agent.act(state)\n",
        "      next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "      done = terminated or truncated\n",
        "      total_reward += reward\n",
        "      next_state = np.reshape(next_state, [1, state_size])\n",
        "      agent.remember(state, action, reward, next_state, done)\n",
        "      state = next_state\n",
        "      if done:\n",
        "          break\n",
        "      if len(agent.memory) > batch_size:\n",
        "          agent.replay(batch_size)\n",
        "    episode_reward_list.append(total_reward)\n",
        "    episode_reward_avg = np.array(episode_reward_list).mean()\n",
        "    print(\"episode: {}/{}, score: {}, e: {:.2}, last 50 ep. avg. rew.: {:.2f}\"\n",
        "                .format(e, EPISODES, total_reward, agent.epsilon, episode_reward_avg))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
