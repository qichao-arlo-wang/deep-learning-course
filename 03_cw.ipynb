{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/03_Network_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUpwc1fHxF4N"
      },
      "source": [
        "# Coursework\n",
        "\n",
        "### Task 1: Tuning a Classification Model\n",
        "In a machine learning problem, and especially when using a deep learning approach, finding the right set of hyperparameters, the right data augmentation strategy, or a good regularization method can make the difference between a model that performs poorly and a model with great accuracy.\n",
        "\n",
        "For this exercise, you will be training a CNN to perform classification in CIFAR-10 (we use the official test set, which is why the variables are called `x_test` and `y_test`, as our validation set) and will analyze the impact of some of the most important elements presented in this tutorial.\n",
        "\n",
        "Use the CNN we give in the code below, along with the given optimizer and number of training epochs as the default setting. Only modify the given CNN architecture to add Dropout or Batch Normalization layers when explicitly stated. Use 40 epochs to plot all of your curves. However, you can train for more epochs to find your best validation performance if your network has not finished training in those 40 epochs.\n",
        "\n",
        "**Report:**\n",
        "*  First, train the given default model without any data augmentation. Then define two data augmentation strategies (one more aggressive than the other) and train the model with data augmentation. Clearly state the two augmentation strategies you apply (i.e., the specific transformations). Discuss the training and validation loss curves for the two data augmentation strategies along with the original run without data augmentation. Attach in the appendix those training and validation curves. Report in a table the best validation accuracy obtained for the three runs (no data augmentation, data augmentation 1, data augmentation 2).\n",
        "\n",
        "*  Without using any data augmentation, analyze the effect of using Dropout in the model. Carry out the same analysis for Batch Normalization. Finally, combine both. Report in the same table as in the data augmentation task the best validation accuracy for each of the three settings (baseline + Dropout, baseline + Batch Normalization, baseline + Batch Normalization + Dropout). The performance will vary depending on where the Dropout layers and Batch Normalization layers are, so state clearly where you added the layers, and what rate you used for the Dropout layers. Discuss the results.\n",
        "\n",
        "* Using the default model/hyperparameters and no data augmentation, report the best validation accuracy when using `zeros` for the kernel initialization. Report the performance in the same table as in the dropout/batch normalization/data augmentation tasks. Discuss the results that you obtained.\n",
        "\n",
        "*  Using the default model and no data augmentation, change the optimizer to SGD and train it with learning rates of `3e-3`, `1e-3` and `3e-4`. Report in a figure the training and validation loss for the three learning rate values and discuss the figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwx-hX-X1HzZ"
      },
      "outputs": [],
      "source": [
        "# Loading CIFAR-10 train set to calculate mean and std for normalizations\n",
        "x = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "loader = torch.utils.data.DataLoader(x, batch_size=len(x), shuffle=False)\n",
        "data_tensor, _ = next(iter(loader))\n",
        "\n",
        "# Compute mean and std per channel\n",
        "mean = data_tensor.mean(dim=[0,2,3])\n",
        "std = data_tensor.std(dim=[0,2,3])\n",
        "\n",
        "mean = mean.tolist()\n",
        "std = std.tolist()\n",
        "\n",
        "print(\"Per-channel mean:\", mean)\n",
        "print(\"Per-channel std:\", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULFyOsS42DeY"
      },
      "outputs": [],
      "source": [
        "# Default: No augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "    # You can later add augmentations like:\n",
        "    # transforms.RandomHorizontalFlip(),\n",
        "    # transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)), etc.\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform)\n",
        "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Image shape:\", np.array(train_dataset[0][0]).shape)\n",
        "print(\"Total number of training samples:\", len(train_dataset))\n",
        "print(\"Total number of validation samples:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqBpuNY42sl7"
      },
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, use_dropout=False, use_batchnorm=False, dropout_rate=0.5, init_mode=\"xavier_uniform\"):\n",
        "        super().__init__()\n",
        "\n",
        "        def conv_block(in_channels, out_channels, use_bn, init, use_dropout, dropout_rate):\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "            if init == \"zeros\":\n",
        "                nn.init.constant_(conv.weight, 0.0)\n",
        "                nn.init.constant_(conv.bias, 0.0)\n",
        "            else:\n",
        "                nn.init.xavier_uniform_(conv.weight)\n",
        "                nn.init.constant_(conv.bias, 0.0)\n",
        "            layers = [conv]\n",
        "            if use_bn:\n",
        "                layers.append(nn.BatchNorm2d(out_channels))\n",
        "            layers.append(nn.ReLU())\n",
        "            if use_dropout:\n",
        "                layers.append(nn.Dropout2d(dropout_rate))\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "        # 4 conv blocks with ReLU and MaxPool2d(padding=1)\n",
        "        self.features = nn.Sequential(\n",
        "            conv_block(3, 32, use_batchnorm, init_mode, use_dropout, dropout_rate),\n",
        "            nn.MaxPool2d(2,2, padding=0),\n",
        "\n",
        "            conv_block(32, 64, use_batchnorm, init_mode, use_dropout, dropout_rate),\n",
        "            nn.MaxPool2d(2,2, padding=0),\n",
        "\n",
        "            conv_block(64, 128, use_batchnorm, init_mode, use_dropout, dropout_rate),\n",
        "            nn.MaxPool2d(2,2, padding=0),\n",
        "\n",
        "            conv_block(128, 256, use_batchnorm, init_mode, use_dropout, dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout_rate if use_dropout else 0.0),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkZejJ3x6lcI"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=40):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct_train += (preds == targets).sum().item()\n",
        "            total_train += targets.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_acc = correct_train / total_train\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct_val = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                correct_val += (preds == targets).sum().item()\n",
        "\n",
        "        val_loss /= len(test_loader.dataset)\n",
        "        val_acc = correct_val / len(test_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{epochs}] \"\n",
        "            f\"- Train Loss: {train_loss:.4f},  Train Acc: {train_acc*100:.2f}%, \"\n",
        "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\"\n",
        "        )\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"train_acc\": train_accuracies,\n",
        "        \"val_loss\": val_losses,\n",
        "        \"val_acc\": val_accuracies\n",
        "    }\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8wS242y6oBs"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Use \"model = CNNModel()\" to get the default model\n",
        "model = CNNModel(use_dropout=False, use_batchnorm=False, init_mode=\"xavier_uniform\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4) # Edit optimizer and learning rate here.\n",
        "\n",
        "history = train_model(model, train_loader, test_loader, criterion, optimizer, epochs=40)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "generative_ai_disabled": true,
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
