{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/05_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwawQJMZ3aQM"
      },
      "source": [
        "# Coursework\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKfTmNC7Bj5E"
      },
      "source": [
        "### **Task 1: RNN Regression**\n",
        "\n",
        "In this task, you are asked to estimate the next value of a time series. Specifically, we have selected the popular airline passenger dataset. This dataset contains the number of passengers that travels with a certain airline company. The data contains 144 entries, each entry corresponds to the number of the passengers that travel in a given month. The dataset starts in 1949, and it lasts until 1960.\n",
        "\n",
        "Similarly to the previous example, we import the data and plot it to see the structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA2fLF24syKo"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRf3UGQ33bnN"
      },
      "outputs": [],
      "source": [
        "# Load dataset (only 2nd column)\n",
        "data = pd.read_csv(\"airline-passengers.csv\", usecols=[1], engine=\"python\")\n",
        "\n",
        "# Plot passengers over time\n",
        "data.plot(title=\"Airline Passengers (1949–1960)\")\n",
        "plt.xlabel(\"Months\")\n",
        "plt.ylabel(\"Passengers\")\n",
        "plt.show()\n",
        "\n",
        "# Convert to float32 numpy\n",
        "data_np = data.to_numpy(dtype=\"float32\")\n",
        "\n",
        "# Train/test split (70/30)\n",
        "split_idx = int(len(data_np) * 0.7)\n",
        "train_np, test_np = data_np[:split_idx], data_np[split_idx:]\n",
        "\n",
        "print(f\"Training samples: {len(train_np)} | Test samples: {len(test_np)}\")\n",
        "\n",
        "# Scale using MinMaxScaler (fit on train, transform both)\n",
        "scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
        "train_np_norm, test_np_norm = scaler.fit_transform(train_np), scaler.transform(test_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOi9udyY3dHS"
      },
      "source": [
        "First of all, you need to train an RNN on the airline passenger dataset. This exercise expects you to study the impact of the `window_size` variable when defining the `train` and `test` dataset splits. Remember that the `window_size` variable indicates the number of past observations used for predicting the current value. Here, we treat the `test` split as a validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIB08JxD3ezu"
      },
      "outputs": [],
      "source": [
        "def create_dataset(dataset, window_size = 1):\n",
        "    data_x, data_y = [], []\n",
        "    for i in range(len(dataset) - window_size):\n",
        "        sample = dataset[i:(i + window_size), 0]\n",
        "        data_x.append(sample)\n",
        "        data_y.append(dataset[i + window_size, 0])\n",
        "    return np.array(data_x), np.array(data_y)\n",
        "\n",
        "\n",
        "window_size = 1 # Use this variable to build the dataset with different number of inputs\n",
        "\n",
        "# Create test and training sets for regression with different window sizes.\n",
        "train_X, train_Y = create_dataset(train_np_norm, window_size)\n",
        "test_X, test_Y = create_dataset(test_np_norm, window_size)\n",
        "\n",
        "train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 1))\n",
        "test_X = np.reshape(test_X, (test_X.shape[0], test_X.shape[1], 1))\n",
        "\n",
        "# Convert to tensors\n",
        "train_X_tensor = torch.tensor(train_X, dtype=torch.float32)\n",
        "train_Y_tensor = torch.tensor(train_Y, dtype=torch.float32).unsqueeze(1)\n",
        "test_X_tensor = torch.tensor(test_X, dtype=torch.float32)\n",
        "test_Y_tensor = torch.tensor(test_Y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "print(\"Shape of training inputs: \" + str((train_X_tensor.shape)))\n",
        "print(\"Shape of training labels: \" + str((train_Y_tensor.shape)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH6W6t5q3gfN"
      },
      "source": [
        "\n",
        "**Report**:\n",
        "\n",
        "*   Create a plot showing the test curves of models trained with different `window_size` values. Report the plot and discuss the main differences you observe between the predicted curves. You can use the style proposed on the Many to One RNNs - Regression section to plot your curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwb9IfiCLKuT"
      },
      "source": [
        "### **Task 2: Text Embeddings**\n",
        "For this task, we tackle a classification problem using the IMDB sentiment dataset as done in the example in the notebook. Labels in IMDB are 0 for negative reviews and 1 for positive reviews. The definitions of the models you will use for this task are given the code below. This task is similar to the transfer learning/finetuning task in the CNN Architectures notebook, however we now test the effect of transfer learning in the embeddings. In this task we use train, validation and test splits with Early Stopping. That means that we will take the best performing model in the validation set and use it in the test set to get a final performance.\n",
        "\n",
        "**Report**\n",
        "* Using embeddings of dimensionality 1, train a model without using any LSTM, only using an average pooling of the input embeddings (called `embeddings_model` in the code given below). Then train another model with an LSTM and trainable embeddings initialized at random (called `lstm_model`). Finally train a model with an LSTM with non-trainable embeddings initialized with GloVe embeddings (called `lstm_glove_model`). The code to train the three models is given below. Report in a table the test accuracy obtained after training with the given code for the three models. Also attach in the Appendix the training and validation accuracy curves for the different models trained. You can report the curves after using EarlyStopping with patience 10 (already given in the code), so you don't have to train for the full 50 epochs the three models. Discuss the results.\n",
        "\n",
        "* Predict the sentiment of the two given example reviews in the code below for the model trained without a LSTM (`embeddings_model`) and for the model trained with a LSTM and GloVe embeddings (`lstm_glove_model`). Report the predictions (you can use the same table as when reporting test accuracies). Discuss the results. Also discuss the differences you can observe between the GloVe embeddings and the embeddings learnt in `embeddings_model` (e.g. what kind of properties the embeddings encode, or differences in the closest words).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gq0lcyLnaSN"
      },
      "source": [
        "We provide the training code you need to use for this exercise below. First we load the dataset as we did in the tutorial. In this exercise, we will use of train, validation and test splits, which are defined in the next cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0ibhzVdnZ0X"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "nb_words = 5000\n",
        "maxlen = 100\n",
        "\n",
        "# Load IMDb dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Tokenizer\n",
        "def tokenize(text):\n",
        "    return nltk.word_tokenize(text.lower())\n",
        "\n",
        "# Build vocabulary from training set\n",
        "counter = Counter()\n",
        "for text in dataset['train']['text']:\n",
        "    counter.update(tokenize(text))\n",
        "\n",
        "# Create word2idx with special tokens\n",
        "most_common = counter.most_common(nb_words - 3)\n",
        "word2idx = {'<PAD>': 0, '<START>': 1, '<UNK>': 2}\n",
        "for idx, (word, _) in enumerate(most_common, start=3):\n",
        "    word2idx[word] = idx\n",
        "\n",
        "# Reverse index\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "# Encode and pad a single text\n",
        "def encode(text, maxlen=maxlen):\n",
        "    tokens = tokenize(text)\n",
        "    indices = [word2idx['<START>']] + [word2idx.get(t, word2idx['<UNK>']) for t in tokens]\n",
        "    indices = indices[:maxlen]  # truncate if too long\n",
        "    indices += [word2idx['<PAD>']] * (maxlen - len(indices))  # pad if too short\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "# Preprocess list of texts\n",
        "def preprocess(text_list):\n",
        "    return torch.stack([encode(text) for text in text_list])\n",
        "\n",
        "# Process datasets\n",
        "x_train_full = preprocess(dataset['train']['text'])\n",
        "y_train_full = torch.tensor(dataset['train']['label'], dtype=torch.long)\n",
        "x_test = preprocess(dataset['test']['text'])\n",
        "y_test = torch.tensor(dataset['test']['label'], dtype=torch.long)\n",
        "\n",
        "# Train/Val split\n",
        "x_val = x_train_full[20000:]\n",
        "y_val = y_train_full[20000:]\n",
        "x_train = x_train_full[:20000]\n",
        "y_train = y_train_full[:20000]\n",
        "\n",
        "# Print shapes\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_val shape:', x_val.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "# Wrap in DataLoaders\n",
        "train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size=32)\n",
        "test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqBseveW25fw"
      },
      "outputs": [],
      "source": [
        "# Function implementing early stopping logic\n",
        "\n",
        "def train_with_early_stopping(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    device,\n",
        "    max_epochs=50,\n",
        "    patience=10,\n",
        "    reshape=False\n",
        "):\n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "    epochs_no_improve = 0\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # --- Training ---\n",
        "        model.train()\n",
        "        train_loss, train_correct, total = 0.0, 0, 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_batch)\n",
        "            if reshape:\n",
        "              loss = criterion(outputs, y_batch.unsqueeze(1).float())\n",
        "            else:\n",
        "              loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * x_batch.size(0)\n",
        "            preds = (torch.sigmoid(outputs) >= 0.5).long()\n",
        "            train_correct += (preds.view(-1) == y_batch.long()).sum().item()\n",
        "            total += x_batch.size(0)\n",
        "\n",
        "        avg_train_loss = train_loss / total\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val in val_loader:\n",
        "                x_val, y_val = x_val.to(device), y_val.to(device).float()\n",
        "                outputs = model(x_val)\n",
        "                if reshape:\n",
        "                  loss = criterion(outputs, y_val.unsqueeze(1).float())\n",
        "                else:\n",
        "                  loss = criterion(outputs, y_val)\n",
        "\n",
        "                val_loss += loss.item() * x_val.size(0)\n",
        "                preds = (torch.sigmoid(outputs) >= 0.5).long()\n",
        "                val_correct += (preds.view(-1) == y_val.long()).sum().item()\n",
        "                val_total += x_val.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss / val_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "        # --- Early stopping logic ---\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "            print(\"  ↳ New best model saved\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    # --- Final test evaluation ---\n",
        "    model.eval()\n",
        "    test_loss, test_correct, test_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test, y_test = x_test.to(device), y_test.to(device).float()\n",
        "            outputs = model(x_test)\n",
        "            if reshape:\n",
        "              loss = criterion(outputs, y_test.unsqueeze(1).float())\n",
        "            else:\n",
        "              loss = criterion(outputs, y_test)\n",
        "\n",
        "            test_loss += loss.item() * x_test.size(0)\n",
        "            preds = (torch.sigmoid(outputs) >= 0.5).long()\n",
        "            test_correct += (preds.view(-1) == y_test.long()).sum().item()\n",
        "            test_total += x_test.size(0)\n",
        "\n",
        "    final_test_loss = test_loss / test_total\n",
        "    final_test_acc = test_correct / test_total\n",
        "\n",
        "    print(f\"Final Test Loss: {final_test_loss:.4f}\")\n",
        "    print(f\"Final Test Accuracy: {final_test_acc:.4f}\")\n",
        "\n",
        "    return history, final_test_loss, final_test_acc, best_model_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EDi21HPGU3h"
      },
      "source": [
        "The following code includes the model that uses embeddings of size 1 (so each word is only represented by a single digit) and averages them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7421cnIYoPQ-"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "class EmbeddingsModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=1):\n",
        "        super(EmbeddingsModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.global_avg_pool = lambda x: x.mean(dim=1)\n",
        "        self.fc = nn.Linear(embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)                     # [batch_size, maxlen, 1]\n",
        "        x = self.global_avg_pool(x)               # [batch_size, 1]\n",
        "        x = self.fc(x)                            # [batch_size, 1]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK0HXie6FN-t"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Instantiate model\n",
        "embeddings_model = EmbeddingsModel(\n",
        "    vocab_size=len(word2idx)\n",
        ").to(DEVICE)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(embeddings_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Call the training function\n",
        "history, test_loss, test_acc, best_embeddings_model_state = train_with_early_stopping(\n",
        "    model=embeddings_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    device=DEVICE,\n",
        "    reshape=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRm3ql3UE9w7"
      },
      "source": [
        "We use Early Stopping, so the best validation model is then used to compute the result in the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_DTYhcl68GF"
      },
      "source": [
        "Now we have `embedding_model` trained. The code below will print the embedding of any `query_word`, which in this case is a single number. We also give you the code to compute the `top_k` closest embeddings to `query_word`. The metric used is the L2 distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9uJoVAt7EWk"
      },
      "outputs": [],
      "source": [
        "def get_most_similar_words(best_model_state, model_class, parameters,\n",
        "                           query, word2idx, idx2word, top_k=10, is_index=False):\n",
        "    \"\"\"\n",
        "    query: can be a word (like 'cat') or an index (like 1 if is_index=True)\n",
        "    \"\"\"\n",
        "    # Load model and set to eval mode\n",
        "    model = model_class(**parameters)\n",
        "    model.load_state_dict(best_model_state)\n",
        "    model.eval()\n",
        "\n",
        "    # Get embeddings\n",
        "    embeddings = model.embedding.weight.data.cpu().numpy()\n",
        "\n",
        "    # Handle query input\n",
        "    if is_index:\n",
        "        query_idx = int(query)\n",
        "        query_word = idx2word.get(query_idx, \"<UNK>\")\n",
        "    else:\n",
        "        query_word = query\n",
        "        if query_word not in word2idx:\n",
        "            print(f\"Word '{query_word}' not found in vocab.\")\n",
        "            return\n",
        "        query_idx = word2idx[query_word]\n",
        "\n",
        "    query_vector = embeddings[query_idx]\n",
        "\n",
        "    # Compute L2 distances\n",
        "    distances = ((embeddings - query_vector) ** 2).sum(axis=1)\n",
        "    nearest_indices = distances.argsort()[1:top_k+1]\n",
        "\n",
        "    print(f\"Query index: {query_idx}\")\n",
        "    print(f\"Query word: '{query_word}'\")\n",
        "    print(f\"Embedding value of '{query_word}' is {query_vector[0]:.6f}\")\n",
        "    print(f\"Most {top_k} similar words to '{query_word}':\")\n",
        "    for rank, idx in enumerate(nearest_indices, start=1):\n",
        "        print(f\"{rank}: {idx2word.get(idx, '<UNK>')}\")\n",
        "\n",
        "# Example usage\n",
        "parameters = {\n",
        "    \"vocab_size\": len(word2idx)\n",
        "}\n",
        "\n",
        "get_most_similar_words(\n",
        "    best_model_state=best_embeddings_model_state,\n",
        "    model_class=EmbeddingsModel,\n",
        "    parameters=parameters,\n",
        "    query='8',            # Query parameter\n",
        "    word2idx=word2idx,\n",
        "    idx2word=idx2word,\n",
        "    top_k=10,\n",
        "    is_index=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhD9hosS7cKQ"
      },
      "source": [
        "The code below gives the prediction for two example reviews we input. Remember that predictions close to 0 refer to a negative review, and predictions close to 1 refer to a positive review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvc9M9jB589r"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text, model_class, best_model_state, word2idx, parameters, maxlen, device='cpu'):\n",
        "    # Re-instantiate and load model\n",
        "    model = model_class(**parameters)\n",
        "    model.load_state_dict(best_model_state)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize and encode\n",
        "    tokens = text.lower().split()\n",
        "    encoded = [word2idx.get('<START>', 1)] + [word2idx.get(w, word2idx['<UNK>']) for w in tokens]\n",
        "    tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0)  # batch of 1\n",
        "\n",
        "    # Pad or truncate\n",
        "    if tensor.size(1) < maxlen:\n",
        "        pad_len = maxlen - tensor.size(1)\n",
        "        tensor = F.pad(tensor, (0, pad_len), value=word2idx['<PAD>'])\n",
        "    else:\n",
        "        tensor = tensor[:, :maxlen]\n",
        "\n",
        "    tensor = tensor.to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        output = model(tensor)\n",
        "        prob = torch.sigmoid(output).item()\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFAWpl-X5_y9"
      },
      "outputs": [],
      "source": [
        "neg_review = \"the movie is boring and not good\"\n",
        "pos_review = \"the movie is good and not boring\"\n",
        "\n",
        "neg_score = predict_sentiment(\n",
        "    text=neg_review,\n",
        "    model_class=EmbeddingsModel,\n",
        "    best_model_state=best_embeddings_model_state,\n",
        "    word2idx=word2idx,\n",
        "    parameters=parameters,\n",
        "    maxlen=100\n",
        ")\n",
        "\n",
        "pos_score = predict_sentiment(\n",
        "    text=pos_review,\n",
        "    model_class=EmbeddingsModel,\n",
        "    best_model_state=best_embeddings_model_state,\n",
        "    word2idx=word2idx,\n",
        "    parameters=parameters,\n",
        "    maxlen=100\n",
        ")\n",
        "\n",
        "print(f\"The score for the negative review is: {neg_score}\")\n",
        "print(f\"The score for the positive review is: {pos_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FzItW89aLVj"
      },
      "source": [
        "With the above code, we trained a model that classifies the sentiment of the sentence using the average of all the embeddings, which were only of size 1. Now we will increase the capacity of the embeddings to 300 and will also add a LSTM to process the embeddings. Hence, the model has a much higher capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-z4K9iAaKR1"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=300, lstm_units=50, dropout=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        ### Do not modify the layers below\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True, num_layers=2, dropout=dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(lstm_units, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)                     # [batch_size, seq_len, embedding_dim]\n",
        "        x = self.dropout1(x)\n",
        "        output, (hidden, _) = self.lstm(x)        # hidden: [1, batch_size, lstm_units]\n",
        "        x = self.dropout2(hidden[-1])             # Take the final hidden state\n",
        "        x = self.fc(x)                            # [batch_size, 1]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msBDHkgfFhtq"
      },
      "source": [
        "Similarly, we use EarlyStopping for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCAyRtjo-s0Z"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Instantiate model\n",
        "lstm_model = LSTMModel(\n",
        "    vocab_size=len(word2idx),\n",
        "    embedding_dim=300,\n",
        "    lstm_units=50\n",
        ").to(DEVICE)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Call the training function\n",
        "history, test_loss, test_acc, best_lstm_state = train_with_early_stopping(\n",
        "    model=lstm_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    device=DEVICE,\n",
        "    reshape=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR31KTPAAqzZ"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    \"vocab_size\": len(word2idx)\n",
        "}\n",
        "\n",
        "neg_review = \"the movie is boring and not good\"\n",
        "pos_review = \"the movie is good and not boring\"\n",
        "\n",
        "neg_score = predict_sentiment(\n",
        "    text=neg_review,\n",
        "    model_class=LSTMModel,\n",
        "    best_model_state=best_lstm_state,\n",
        "    word2idx=word2idx,\n",
        "    parameters=parameters,\n",
        "    maxlen=100\n",
        ")\n",
        "\n",
        "pos_score = predict_sentiment(\n",
        "    text=pos_review,\n",
        "    model_class=LSTMModel,\n",
        "    best_model_state=best_lstm_state,\n",
        "    word2idx=word2idx,\n",
        "    parameters=parameters,\n",
        "    maxlen=100\n",
        ")\n",
        "\n",
        "print(f\"The score for the negative review is: {neg_score}\")\n",
        "print(f\"The score for the positive review is: {pos_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3F0IRz2FqVX"
      },
      "source": [
        "We just trained a model with a large number of parameters in the IMDB, which is a small dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7dnHR3za1xB"
      },
      "source": [
        "The last model we train is the same model as the `lstm_model` above, but in this case we use the embeddings from the GloVe method (which were introduced in this notebook) without any finetuning. First, we download them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7ZjGnuWb4H9"
      },
      "outputs": [],
      "source": [
        "!wget https://imperialcollegelondon.box.com/shared/static/c9trfhhwl9ohje5g3sapu3xk2zoywp3c.txt -O glove_vectors.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vhK-7g7GIZX"
      },
      "source": [
        "Then we load the GloVe embeddings with dimensionality 300 we just downloaded. This takes some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XzERyfEC-sl"
      },
      "outputs": [],
      "source": [
        "# Load GloVe vectors and build vocab and weight matrix\n",
        "def load_glove(glove_path, embedding_dim):\n",
        "    vocab = {}\n",
        "    vectors = []\n",
        "    skipped = 0\n",
        "\n",
        "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        first_line = f.readline()\n",
        "        if len(first_line.strip().split()) == 2:\n",
        "            print(f\"Skipping header: {first_line.strip()}\")\n",
        "        else:\n",
        "            f.seek(0)\n",
        "\n",
        "        for line in f:\n",
        "            tokens = line.rstrip().split()\n",
        "            word, values = tokens[0], tokens[1:]\n",
        "\n",
        "            if len(values) != embedding_dim:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                vector = np.array(values, dtype=np.float32)\n",
        "            except ValueError:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            vocab[word] = len(vectors)\n",
        "            vectors.append(torch.from_numpy(vector))\n",
        "\n",
        "    if not vectors:\n",
        "        raise RuntimeError(\"No valid embeddings loaded. Check file format.\")\n",
        "\n",
        "    weight_matrix = torch.stack(vectors)\n",
        "    print(f\"Loaded {len(vectors)} word vectors. Skipped {skipped} lines.\")\n",
        "    return vocab, weight_matrix\n",
        "\n",
        "# Example\n",
        "glove_path = \"glove_vectors.txt\"\n",
        "vocab, weight_matrix = load_glove(glove_path, embedding_dim=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0kqDg7ocd5t"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 300\n",
        "embedding_matrix = torch.zeros((nb_words, embedding_dim))\n",
        "\n",
        "# Align GloVe vectors to our vocabulary\n",
        "for word, idx in word2idx.items():\n",
        "    if idx >= nb_words:\n",
        "        continue  # Skip words beyond vocab limit\n",
        "    if word in vocab:\n",
        "        embedding_matrix[idx] = weight_matrix[vocab[word]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXEgmD5CdbEo"
      },
      "source": [
        "To initialize the PyTorch Embedding layer with the embeddings we loaded, we can use the function `nn.Embedding.from_pretrained`. Also, to freeze the embeddings during training, we use `freeze=freeze`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeKMwpxIDd5A"
      },
      "outputs": [],
      "source": [
        "class LSTMWithGloVe(nn.Module):\n",
        "    def __init__(self, embedding_tensor, lstm_units=50, dropout=0.2, freeze=True):\n",
        "        super(LSTMWithGloVe, self).__init__()\n",
        "\n",
        "        # Define embedding layer from pretrained tensor\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=freeze)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_tensor.shape[1],  # Embedding dim\n",
        "            hidden_size=lstm_units,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            num_layers=2\n",
        "        )\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(lstm_units, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)                     # [batch_size, seq_len, embedding_dim]\n",
        "        x = self.dropout1(x)\n",
        "        output, (hidden, _) = self.lstm(x)        # hidden: [1, batch_size, lstm_units]\n",
        "        x = self.dropout2(hidden[-1])             # Final hidden state\n",
        "        x = self.fc(x)                            # [batch_size, 1]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqNwoQE9D69E"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Instantiate model\n",
        "lstm_glove_model = LSTMWithGloVe(\n",
        "    embedding_tensor=embedding_matrix,\n",
        "    lstm_units=50\n",
        ").to(DEVICE)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(lstm_glove_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Call the training function\n",
        "history, test_loss, test_acc, best_lstm_glove_state = train_with_early_stopping(\n",
        "    model=lstm_glove_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    device=DEVICE,\n",
        "    reshape=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0v3mUULGkfV"
      },
      "source": [
        "We can also compute the closest words in the GloVe embeddings to any `query_word` using the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIJqzFFBgdJM"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    \"embedding_tensor\": embedding_matrix\n",
        "}\n",
        "\n",
        "get_most_similar_words(\n",
        "    best_model_state=best_lstm_glove_state,\n",
        "    model_class=LSTMWithGloVe,\n",
        "    parameters=parameters,\n",
        "    query='8',              # Query parameter\n",
        "    word2idx=word2idx,\n",
        "    idx2word=idx2word,\n",
        "    top_k=10,\n",
        "    is_index=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXiBKBgtG2xf"
      },
      "source": [
        "We use the same example reviews as for the `EmbeddingsModel` case and we compute the predictions using the `LSTMWithGloVe`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvGmp625hr39"
      },
      "outputs": [],
      "source": [
        "neg_review = \"the movie is boring and not good\"\n",
        "pos_review = \"the movie is good and not boring\"\n",
        "\n",
        "neg_score = predict_sentiment(\n",
        "    text=neg_review,\n",
        "    model_class=LSTMWithGloVe,\n",
        "    best_model_state=best_lstm_glove_state,\n",
        "    word2idx=word2idx,\n",
        "    parameters=parameters,\n",
        "    maxlen=100\n",
        ")\n",
        "\n",
        "pos_score = predict_sentiment(\n",
        "    text=pos_review,\n",
        "    model_class=LSTMWithGloVe,\n",
        "    best_model_state=best_lstm_glove_state,\n",
        "    word2idx=word2idx,\n",
        "    parameters=parameters,\n",
        "    maxlen=100\n",
        ")\n",
        "\n",
        "print(f\"The score for the negative review is: {neg_score}\")\n",
        "print(f\"The score for the positive review is: {pos_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CywiPmy1eZoF"
      },
      "source": [
        "### **Task 3: Text Generation**\n",
        "In this task we focus on the text generation problem. For this purpose, we will download the scripts of the TV show Game of Thrones and try to generate some text resembling the style of the scripts.\n",
        "\n",
        "\n",
        "**Report**\n",
        "* Plot the retrieved BLEU for different temperature values (from 0 to 2 in the x-axis) for both the character-level model and the word-level model. To compute the BLEU score, use a minimum of 20 generated samples per temperature used to reduce variability (you can increase it at the cost of higher computational time for lower variability). Each sample should contain 100 characters for the char-level model or 30 words for the word-level model (the code given uses these parameters by default). Do you see any relationship between the obtained BLEU score and temperature used? If you generate sentences at different temperatures what differences can you observe? Are the generated sentences grammatically correct? Do the generated sentences make sense?\n",
        "\n",
        "We give below the code needed to download the dataset and to compute the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD9gEFKXqukY"
      },
      "source": [
        "We first download and read the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_I1pRickXKl"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/shekharkoirala/Game_of_Thrones\n",
        "\n",
        "data = open('./Game_of_Thrones/Data/final_data.txt', 'r').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHqzKZQ8aYkp"
      },
      "source": [
        "**Character-level model**\n",
        "\n",
        "We first include the code to build the character-level dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnwDgj_unBhW"
      },
      "outputs": [],
      "source": [
        "# Vocabulary\n",
        "characters = sorted(set(data))\n",
        "n_to_char = {i: ch for i, ch in enumerate(characters)}\n",
        "char_to_n = {ch: i for i, ch in enumerate(characters)}\n",
        "\n",
        "# Sliding windows\n",
        "seq_char_length = 100\n",
        "x_char = np.array([\n",
        "    [char_to_n[ch] for ch in data[i:i+seq_char_length]]\n",
        "    for i in range(len(data) - seq_char_length)\n",
        "], dtype=np.int64)\n",
        "\n",
        "y_char = np.array([\n",
        "    char_to_n[data[i+seq_char_length]]\n",
        "    for i in range(len(data) - seq_char_length)\n",
        "], dtype=np.int64)\n",
        "\n",
        "print(\"Total Samples:\", len(x_char))\n",
        "print(\"x_char shape:\", x_char.shape, \"y_char shape:\", y_char.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcmI8iV7afK5"
      },
      "source": [
        "The splits used for training are given below, although we already give the model trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VkGsFkMnCuB"
      },
      "outputs": [],
      "source": [
        "# Sizes\n",
        "n_samples = len(x_char)\n",
        "n_samples_train = int(n_samples * 0.7)\n",
        "n_samples_test  = int(n_samples * 0.2)\n",
        "n_samples_val   = n_samples - n_samples_train - n_samples_test\n",
        "\n",
        "# Train/val/test splits\n",
        "x_train_char = x_char[:n_samples_train]\n",
        "y_train_char = y_char[:n_samples_train]\n",
        "\n",
        "x_val_char   = x_char[n_samples_train:n_samples_train + n_samples_val]\n",
        "y_val_char   = y_char[n_samples_train:n_samples_train + n_samples_val]\n",
        "\n",
        "x_test_char  = x_char[n_samples_train + n_samples_val:]\n",
        "y_test_char  = y_char[n_samples_train + n_samples_val:]\n",
        "\n",
        "# Convert all to torch tensors in one go\n",
        "to_tensor = lambda arr: torch.tensor(arr, dtype=torch.long)\n",
        "\n",
        "x_train_char, y_train_char = map(to_tensor, (x_train_char, y_train_char))\n",
        "x_val_char,   y_val_char   = map(to_tensor, (x_val_char, y_val_char))\n",
        "x_test_char,  y_test_char  = map(to_tensor, (x_test_char, y_test_char))\n",
        "\n",
        "print(f\"x_train_char: {x_train_char.shape}, y_train_char: {y_train_char.shape}\")\n",
        "print(f\"x_val_char:   {x_val_char.shape},   y_val_char:   {y_val_char.shape}\")\n",
        "print(f\"x_test_char:  {x_test_char.shape},  y_test_char:  {y_test_char.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcyzP955aukz"
      },
      "source": [
        "The definition of the model is the one given below. You will not train the model, so this piece of code is only for you to know what kind of model we trained for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO3FoRM1nEOe"
      },
      "outputs": [],
      "source": [
        "# define the LSTM model\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size=300, lstm_units=256):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, lstm_units, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_units, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)                 # (B, T, E)\n",
        "        out, _ = self.lstm(emb)                 # (B, T, H)\n",
        "        last = out[:, -1, :]                    # (B, H)  last time step\n",
        "        logits = self.fc(last)                  # (B, V)\n",
        "        return logits                           # raw logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSgGhwym_WLx"
      },
      "source": [
        "As the training takes a while, we include a saved model that you can load to skip the training step. Use this model to compute your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J6m8nQmt1eO"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "CODE USED FOR TRAINING (DO NOT RUN IT!)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# ---- config ----\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "patience = 10\n",
        "lr = 1e-3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "vocab_size = len(characters)\n",
        "model = CharLSTM(vocab_size=vocab_size, embedding_size=300, lstm_units=256).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# ---- data loaders (labels are integer class ids) ----\n",
        "train_ds = TensorDataset(x_train_char, y_train_char)\n",
        "val_ds   = TensorDataset(x_val_char,   y_val_char)\n",
        "test_ds  = TensorDataset(x_test_char,  y_test_char)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n",
        "\n",
        "# ---- early stopping ----\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, mode='max', min_delta=0.0, restore_best=True):\n",
        "        self.patience = patience\n",
        "        self.mode = mode  # 'max' for accuracy\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best = restore_best\n",
        "        self.best_score = None\n",
        "        self.counter = 0\n",
        "        self.best_state = None\n",
        "        self.best_epoch = 0\n",
        "\n",
        "    def step(self, score, model, epoch):\n",
        "        improve = False\n",
        "        if self.best_score is None:\n",
        "            improve = True\n",
        "        else:\n",
        "            if self.mode == 'max':\n",
        "                improve = score > (self.best_score + self.min_delta)\n",
        "            else:\n",
        "                improve = score < (self.best_score - self.min_delta)\n",
        "\n",
        "        if improve:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "            if self.restore_best:\n",
        "                self.best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
        "            self.best_epoch = epoch\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            return self.counter > self.patience  # stop if patience exceeded\n",
        "\n",
        "    def restore(self, model):\n",
        "        if self.restore_best and self.best_state is not None:\n",
        "            model.load_state_dict(self.best_state)\n",
        "\n",
        "early_stop = EarlyStopping(patience=patience, mode='max', min_delta=0.0, restore_best=True)\n",
        "\n",
        "# ---- history ----\n",
        "history = {\n",
        "    'loss': [],\n",
        "    'val_loss': [],\n",
        "    'accuracy': [],\n",
        "    'val_accuracy': []\n",
        "}\n",
        "\n",
        "def accuracy_from_logits(logits, targets):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "# ---- training loop ----\n",
        "best_path = 'char_gen_model.pth'\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # train\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)            # [B, V]\n",
        "        loss = criterion(logits, yb)  # yb: [B] integer targets\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc  += accuracy_from_logits(logits, yb)\n",
        "        n_batches  += 1\n",
        "\n",
        "    train_loss = epoch_loss / n_batches\n",
        "    train_acc  = epoch_acc  / n_batches\n",
        "\n",
        "    # validate\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_acc  = 0.0\n",
        "    n_val    = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            val_loss += loss.item()\n",
        "            val_acc  += accuracy_from_logits(logits, yb)\n",
        "            n_val    += 1\n",
        "\n",
        "    val_loss /= max(1, n_val)\n",
        "    val_acc  /= max(1, n_val)\n",
        "\n",
        "    # record history\n",
        "    history['loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['accuracy'].append(train_acc)\n",
        "    history['val_accuracy'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch:3d}/{epochs}  \"\n",
        "          f\"loss={train_loss:.4f}  acc={train_acc:.4f}  \"\n",
        "          f\"val_loss={val_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    # early stopping on val_accuracy\n",
        "    stop = early_stop.step(val_acc, model, epoch)\n",
        "\n",
        "    # save best\n",
        "    if early_stop.best_state is not None and early_stop.best_epoch == epoch:\n",
        "        torch.save(early_stop.best_state, best_path)\n",
        "\n",
        "    if stop:\n",
        "        print(f\"Early stopping at epoch {epoch}. Restoring best epoch {early_stop.best_epoch}...\")\n",
        "        break\n",
        "\n",
        "# restore best weights\n",
        "early_stop.restore(model)\n",
        "torch.save(model.state_dict(), best_path)\n",
        "\n",
        "# ---- test evaluation ----\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "test_acc = 0.0\n",
        "n_test = 0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        test_loss += loss.item()\n",
        "        test_acc  += accuracy_from_logits(logits, yb)\n",
        "        n_test    += 1\n",
        "\n",
        "test_loss /= max(1, n_test)\n",
        "test_acc  /= max(1, n_test)\n",
        "\n",
        "print(f\"\\nFinal test loss is: {test_loss:.4f}\")\n",
        "print(f\"Final test accuracy is: {test_acc:.4f}\")\n",
        "print(f\"Best model saved to: {best_path}\")\n",
        "files.download(best_path)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr7SMrHtfim_"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "!wget 'https://raw.githubusercontent.com/MatchLab-Imperial/deep-learning-course/master/asset/05_RNN/char_gen_model.pth' -O char_gen_model.pth\n",
        "\n",
        "vocab_size = len(characters)\n",
        "model = CharLSTM(vocab_size=vocab_size, embedding_size=300, lstm_units=256).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"char_gen_model.pth\", map_location=DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsuUE8vua77e"
      },
      "source": [
        "The code you need to evaluate the BLEU score is given below. Vary the temperature to the different needed values. It takes around 1 minute in average per temperature if `n_eval` is set to 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5ufMSdgp0A6"
      },
      "outputs": [],
      "source": [
        "characters = sorted(list(set(data)))\n",
        "n_to_char = {n: char for n, char in enumerate(characters)}\n",
        "char_to_n = {char: n for n, char in enumerate(characters)}\n",
        "\n",
        "# Vary the temperature here\n",
        "temperature = 0.5\n",
        "n_eval = 20\n",
        "seq_char_length = 100\n",
        "smoother = SmoothingFunction().method1\n",
        "bleu_score = 0.0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(n_eval):\n",
        "      # Randomly select a starting point in the test data\n",
        "      start = np.random.randint(0, len(x_test_char) - seq_char_length - 1)\n",
        "      pattern = x_test_char[start].tolist()\n",
        "      reference = x_test_char[start + seq_char_length].tolist()\n",
        "\n",
        "      # Convert reference from numbers to characters\n",
        "      reference = ''.join([n_to_char[value] for value in reference])\n",
        "\n",
        "      # Generate characters using the model\n",
        "      output_sent = ''\n",
        "      for _n in range(seq_char_length):\n",
        "          x = torch.tensor(pattern, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "          logits = model(x)\n",
        "          temp = float(temperature) + 0.01\n",
        "          probs = F.softmax((logits + 1e-7) / temp, dim=-1)\n",
        "          idx = torch.multinomial(probs[0], num_samples=1).item()\n",
        "          output_sent += n_to_char[idx]\n",
        "          pattern.append(idx)\n",
        "          pattern = pattern[1:]\n",
        "\n",
        "      # Preprocess reference and candidate text\n",
        "      reference = word_tokenize(reference.lower())\n",
        "      candidate = word_tokenize(output_sent.lower())\n",
        "      reference = list(filter(lambda x: x != '', reference))\n",
        "      candidate = list(filter(lambda x: x != '', candidate))\n",
        "\n",
        "      # Remove incomplete words at the beginning and end of both lists\n",
        "      if len(reference) > 2:\n",
        "          reference = reference[1:-1]\n",
        "      if len(candidate) > 2:\n",
        "          candidate = candidate[1:-1]\n",
        "\n",
        "      # Compute BLEU score\n",
        "      bleu_score += sentence_bleu([reference], candidate, smoothing_function=smoother)\n",
        "\n",
        "bleu_score /= n_eval\n",
        "print(\"BLEU Score:\", bleu_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVEF_WPycOnD"
      },
      "source": [
        "The code below allows you to generate sentences for different input patterns and different temperature values. You can test how the temperature values affect the quality of the output sentences for the character-level model by generating a few examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yshz7OZGnIWX"
      },
      "outputs": [],
      "source": [
        "# Change the temperature here\n",
        "temperature = 0.7\n",
        "seed_text = \"TYRION pours himself some wine and drinks it down. He pours another glass, and walks back to CERSEI \"\n",
        "pattern = [char_to_n[ch] for ch in seed_text[:seq_char_length]]\n",
        "\n",
        "print(\"\\nPredicted:\")\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(300):\n",
        "        x = torch.tensor(pattern, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "        logits = model(x)\n",
        "\n",
        "        if temperature == 0:\n",
        "            idx = torch.argmax(logits, dim=-1).item()\n",
        "        else:\n",
        "            probs = F.softmax(logits / temperature, dim=-1)\n",
        "            idx = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "        ch = n_to_char[idx]\n",
        "        sys.stdout.write(ch)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        pattern = pattern[1:] + [idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeNwJWRN0Mue"
      },
      "source": [
        "**Word-level model**\n",
        "\n",
        "We now give the code to run the word-level model. The code is similar to the char-level model. The main difference is that we only try to predict the 2000 words most commonly used in the dataset. The reason for this limitation is to limit the size of the output layer and number of input embeddings for memory constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-XfzutEv9Ta"
      },
      "outputs": [],
      "source": [
        "n_words = 2000\n",
        "seq_length = 30\n",
        "\n",
        "# Preprocess text: lowercase + space around punctuation\n",
        "data_p = (\n",
        "    data.replace('.', ' . ').replace(',', ' , ').replace(':', ' : ')\n",
        "        .replace('?', ' ? ').replace('!', ' ! ')\n",
        "        .replace('\\n', ' \\n ').replace('[', ' [ ').replace(']', ' ] ')\n",
        "        .replace(')', ' ) ').replace('(', ' ( ').lower().split()\n",
        ")\n",
        "data_p = [tok for tok in data_p if tok.strip()]\n",
        "\n",
        "# Build vocab (most common words)\n",
        "common = [w for w, _ in Counter(data_p).most_common(n_words)]\n",
        "word_to_n = {w: i for i, w in enumerate(common)}\n",
        "n_to_word = {i: w for i, w in enumerate(common)}\n",
        "OOV_IDX = len(word_to_n)\n",
        "\n",
        "# Build dataset\n",
        "x_word, y_word = [], []\n",
        "for i in range(len(data_p) - seq_length):\n",
        "    seq = [word_to_n.get(w, OOV_IDX) for w in data_p[i:i+seq_length]]\n",
        "    label = data_p[i + seq_length]\n",
        "    if label in word_to_n:  # only predict in-vocab words\n",
        "        x_word.append(seq)\n",
        "        y_word.append(word_to_n[label])\n",
        "\n",
        "n_samples = len(x_word)\n",
        "print(\"Total Samples:\", n_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teAOmchqxvQd"
      },
      "outputs": [],
      "source": [
        "n_samples = len(x_word)\n",
        "n_samples_train = int(n_samples * 0.7)\n",
        "n_samples_test  = int(n_samples * 0.2)\n",
        "n_samples_val   = n_samples - n_samples_train - n_samples_test\n",
        "\n",
        "# Train / val / test splits\n",
        "x_train_word = x_word[:n_samples_train]\n",
        "y_train_word = y_word[:n_samples_train]\n",
        "\n",
        "x_val_word   = x_word[n_samples_train:n_samples_train+n_samples_val]\n",
        "y_val_word   = y_word[n_samples_train:n_samples_train+n_samples_val]\n",
        "\n",
        "x_test_word  = x_word[n_samples_train+n_samples_val:]\n",
        "y_test_word  = y_word[n_samples_train+n_samples_val:]\n",
        "\n",
        "# Convert labels to numpy first\n",
        "y_train_word = np.array(y_train_word)\n",
        "y_val_word   = np.array(y_val_word)\n",
        "y_test_word  = np.array(y_test_word)\n",
        "\n",
        "# Convert all to tensors\n",
        "x_train_word = torch.tensor(x_train_word, dtype=torch.long)\n",
        "x_val_word   = torch.tensor(x_val_word,   dtype=torch.long)\n",
        "x_test_word  = torch.tensor(x_test_word,  dtype=torch.long)\n",
        "\n",
        "y_train_word = torch.tensor(y_train_word, dtype=torch.long)\n",
        "y_val_word   = torch.tensor(y_val_word,   dtype=torch.long)\n",
        "y_test_word  = torch.tensor(y_test_word,  dtype=torch.long)\n",
        "\n",
        "# Shapes\n",
        "print(f\"x_train_word: {x_train_word.shape}, y_train_word: {y_train_word.shape}\")\n",
        "print(f\"x_val_word:   {x_val_word.shape},   y_val_word:   {y_val_word.shape}\")\n",
        "print(f\"x_test_word:  {x_test_word.shape},  y_test_word:  {y_test_word.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJvV2NaP0yam"
      },
      "source": [
        "The definition of the word-level model we train is given below. The model is the same as in the char-level case, the only difference is the size of the output vector and the number of input embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myJRka3BxyUf"
      },
      "outputs": [],
      "source": [
        "# define the LSTM model\n",
        "class WordLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size=300, lstm_units=256):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, lstm_units, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_units, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)           # (B, T, E)\n",
        "        out, _ = self.lstm(emb)           # (B, T, H)\n",
        "        last = out[:, -1, :]              # (B, H) last timestep\n",
        "        logits = self.fc(last)            # (B, V)\n",
        "        return logits                     # raw logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGaHG_ZfoaxC"
      },
      "source": [
        "As with the character-level model, training the word-level model takes a while. Use the saved model we included to compute your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcw8vzdQyROI"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "CODE USED FOR TRAINING (DO NOT RUN IT!)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# ---- config ----\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "patience = 10\n",
        "lr = 1e-3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "vocab_size = n_words + 1\n",
        "model = WordLSTM(vocab_size=vocab_size, embedding_size=300, lstm_units=256).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# ---- data loaders (labels are integer class ids) ----\n",
        "train_ds = TensorDataset(x_train_word, y_train_word)\n",
        "val_ds   = TensorDataset(x_val_word,   y_val_word)\n",
        "test_ds  = TensorDataset(x_test_word,  y_test_word)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n",
        "\n",
        "# ---- early stopping ----\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, mode='max', min_delta=0.0, restore_best=True):\n",
        "        self.patience = patience\n",
        "        self.mode = mode  # 'max' for accuracy\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best = restore_best\n",
        "        self.best_score = None\n",
        "        self.counter = 0\n",
        "        self.best_state = None\n",
        "        self.best_epoch = 0\n",
        "\n",
        "    def step(self, score, model, epoch):\n",
        "        improve = False\n",
        "        if self.best_score is None:\n",
        "            improve = True\n",
        "        else:\n",
        "            if self.mode == 'max':\n",
        "                improve = score > (self.best_score + self.min_delta)\n",
        "            else:\n",
        "                improve = score < (self.best_score - self.min_delta)\n",
        "\n",
        "        if improve:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "            if self.restore_best:\n",
        "                self.best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
        "            self.best_epoch = epoch\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            return self.counter > self.patience  # stop if patience exceeded\n",
        "\n",
        "    def restore(self, model):\n",
        "        if self.restore_best and self.best_state is not None:\n",
        "            model.load_state_dict(self.best_state)\n",
        "\n",
        "early_stop = EarlyStopping(patience=patience, mode='max', min_delta=0.0, restore_best=True)\n",
        "\n",
        "# ---- history ----\n",
        "history = {\n",
        "    'loss': [],\n",
        "    'val_loss': [],\n",
        "    'accuracy': [],\n",
        "    'val_accuracy': []\n",
        "}\n",
        "\n",
        "def accuracy_from_logits(logits, targets):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "# ---- training loop ----\n",
        "best_path = 'word_gen_model.pth'\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # train\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)            # [B, V]\n",
        "        loss = criterion(logits, yb)  # yb: [B] integer targets\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc  += accuracy_from_logits(logits, yb)\n",
        "        n_batches  += 1\n",
        "\n",
        "    train_loss = epoch_loss / n_batches\n",
        "    train_acc  = epoch_acc  / n_batches\n",
        "\n",
        "    # validate\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_acc  = 0.0\n",
        "    n_val    = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            val_loss += loss.item()\n",
        "            val_acc  += accuracy_from_logits(logits, yb)\n",
        "            n_val    += 1\n",
        "\n",
        "    val_loss /= max(1, n_val)\n",
        "    val_acc  /= max(1, n_val)\n",
        "\n",
        "    # record history\n",
        "    history['loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['accuracy'].append(train_acc)\n",
        "    history['val_accuracy'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch:3d}/{epochs}  \"\n",
        "          f\"loss={train_loss:.4f}  acc={train_acc:.4f}  \"\n",
        "          f\"val_loss={val_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    # early stopping on val_accuracy\n",
        "    stop = early_stop.step(val_acc, model, epoch)\n",
        "    if early_stop.best_state is not None and early_stop.best_epoch == epoch:\n",
        "        torch.save(early_stop.best_state, best_path)\n",
        "\n",
        "    if stop:\n",
        "        print(f\"Early stopping at epoch {epoch}. Restoring best epoch {early_stop.best_epoch}...\")\n",
        "        break\n",
        "\n",
        "# restore best weights\n",
        "early_stop.restore(model)\n",
        "torch.save(model.state_dict(), best_path)\n",
        "\n",
        "# ---- test evaluation ----\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "test_acc = 0.0\n",
        "n_test = 0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        test_loss += loss.item()\n",
        "        test_acc  += accuracy_from_logits(logits, yb)\n",
        "        n_test    += 1\n",
        "\n",
        "test_loss /= max(1, n_test)\n",
        "test_acc  /= max(1, n_test)\n",
        "\n",
        "print(f\"\\nFinal test loss is: {test_loss:.4f}\")\n",
        "print(f\"Final test accuracy is: {test_acc:.4f}\")\n",
        "print(f\"Best model saved to: {best_path}\")\n",
        "files.download(best_path)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql_o1vFO5rFp"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "!wget 'https://raw.githubusercontent.com/MatchLab-Imperial/deep-learning-course/master/asset/05_RNN/word_gen_model.pth' -O word_gen_model.pth\n",
        "\n",
        "vocab_size = n_words + 1\n",
        "model = WordLSTM(vocab_size=vocab_size, embedding_size=300, lstm_units=256).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"word_gen_model.pth\", map_location=DEVICE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgvJiBOs5CEF"
      },
      "outputs": [],
      "source": [
        "# Vary the temperature here\n",
        "temperature = 0.7\n",
        "n_eval = 20\n",
        "seq_char_length = 100\n",
        "smoother = SmoothingFunction().method1\n",
        "bleu_score = 0.0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(n_eval):\n",
        "      # We look for references that do not contain any non-common words as we only\n",
        "      # learnt to predict the 2000 most common words\n",
        "      while True:\n",
        "          start = np.random.randint(0, len(x_test_word)-seq_length-1)\n",
        "          pattern = x_test_word[start].tolist()\n",
        "          reference = x_test_word[start+seq_length].tolist()\n",
        "          if n_words not in reference:\n",
        "              break\n",
        "      reference = ' '.join([n_to_word[value] for value in reference])\n",
        "\n",
        "      # generate words\n",
        "      output_sent = ''\n",
        "      for i in range(seq_length):\n",
        "          x = torch.tensor(pattern, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "          logits = model(x)\n",
        "          probs = F.softmax((logits + 1e-7) / (float(temperature) + 0.01), dim=-1)\n",
        "          idx = torch.multinomial(probs[0], num_samples=1).item()\n",
        "\n",
        "          word = n_to_word.get(idx, '')\n",
        "          output_sent += word + ' '\n",
        "\n",
        "          pattern.append(idx)\n",
        "          pattern = pattern[1:]\n",
        "\n",
        "      # Preprocess reference and candidate text\n",
        "      reference = word_tokenize(reference.lower())\n",
        "      candidate = word_tokenize(output_sent.lower())\n",
        "\n",
        "      # Remove empty strings (if any) after tokenization\n",
        "      reference = list(filter(lambda x: x != '', reference))\n",
        "      candidate = list(filter(lambda x: x != '', candidate))\n",
        "\n",
        "      # Remove incomplete words at the beginning and end of both lists\n",
        "      if len(reference) > 2:\n",
        "          reference = reference[1:-1]\n",
        "      if len(candidate) > 2:\n",
        "          candidate = candidate[1:-1]\n",
        "\n",
        "      # Compute BLEU score for the candidate and reference\n",
        "      bleu_score += sentence_bleu([reference], candidate, smoothing_function=smoother)\n",
        "\n",
        "bleu_score /= n_eval\n",
        "print(\"BLEU Score:\", bleu_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf1cYUeC27W3"
      },
      "outputs": [],
      "source": [
        "# Vary the temperature here\n",
        "temperature = 0.7\n",
        "seed_text = (\n",
        "    \"TYRION pours himself some wine and drinks it down. He pours another glass, \"\n",
        "    \"and walks back to CERSEI placing his cup on her desk. He takes another glass.\\n\"\n",
        "    \"TYRION: \"\n",
        ")\n",
        "\n",
        "# Preprocess seed\n",
        "pattern = (\n",
        "    seed_text.replace('.', ' . ').replace(',', ' , ').replace(':', ' : ')\n",
        "    .replace('?', ' ? ').replace('!', ' ! ')\n",
        "    .replace('\\n', ' \\n ').replace('[', ' [ ').replace(']', ' ] ')\n",
        "    .replace(')', ' ) ').replace('(', ' ( ')\n",
        "    .lower()\n",
        "    .split()\n",
        ")\n",
        "pattern = [w for w in pattern if w.strip()][:seq_length]\n",
        "print(\"\\nInput Pattern:\\n\", \" \".join(pattern))\n",
        "\n",
        "# Map to indices (OOV → n_words)\n",
        "pattern = [word_to_n.get(w, n_words) for w in pattern]\n",
        "\n",
        "print(\"\\nPredicted:\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for _ in range(100):\n",
        "        x = torch.tensor(pattern, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "        logits = model(x)\n",
        "\n",
        "        if temperature == 0:\n",
        "            idx = torch.argmax(logits, dim=-1).item()\n",
        "        else:\n",
        "            probs = F.softmax(logits / temperature, dim=-1)\n",
        "            idx = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "        word = n_to_word.get(idx, \"<UNK>\")\n",
        "        sys.stdout.write(word + \" \")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Update pattern\n",
        "        pattern = pattern[1:] + [idx]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
