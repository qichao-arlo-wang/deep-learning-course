{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/06_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w3Zw02rsdMK"
      },
      "source": [
        "# Coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrJfQmqvASYC"
      },
      "source": [
        "## Task 1: Non-linear Transformations for Representation Learning\n",
        "\n",
        "PCA is a standard dimensionality reduction technique that uses a linear transformation. In this task we are going to define two autoencoders, one convolutional and one without using any convolutional layer, that are capable of learning a non-linear transformation to reduce the dimensionality of the input MNIST image, and we will compare those autoencoders to PCA. A way to evaluate the quality of the representations produced by both PCA and the autoencoder is to learn a classifier on top of those representations with reduced dimensionality. If the classifier has high accuracy, then the representations can be considered meaningful. In our case, we will use representations with dimensionality 10 and we will use those representations to train a linear classifier, which is defined in the code below.\n",
        "\n",
        "The given example architectures for both the non-convolutional and the convolutional autoencoder already produce, after training them, representations of similar quality as PCA. Modify the given architectures and try to increase the accuracy when training a linear classifier on top of the autoencoder representations. The code given below may help you understand the pipeline.\n",
        "\n",
        "As in past notebooks, treat the MNIST test set as your validation set. You can use any of the layers and techniques presented in past notebooks, the only constraints are that the non-convolutional autoencoder should not have any Conv2d layer, that the convolutional autoencoder should include Conv2d layers, and that the representation vector should have dimensionality 10.\n",
        "\n",
        "**Report**:\n",
        "* Table with the accuracy of `classifier` (defined below) obtained with the representations from your two proposed  autoencoder architectures (non-convolutional and convolutional autoencoder) and also with PCA with 10 components in the training set and the validation set. Additionally, include in the table the MSE error in both training and validation set for your non-convolutional autoencoder, for your convolutional autoencoder and for the PCA method. State clearly your two final autoencoder architectures and discuss the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUSZnVLp1kq8"
      },
      "source": [
        "We will use MNIST for this task. First, we resize all the images to have a resolution of 32x32, which will make the definition of the convolutional autoencoder easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVL-ur3qr74_"
      },
      "outputs": [],
      "source": [
        "# Fix seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Load MNIST and preprocess: normalize and resize to 32x32 with 1 channel\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "def resize_dataset(dataset):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    for img, label in dataset:\n",
        "        # img shape (1,28,28), convert to numpy\n",
        "        img_np = img.numpy().transpose(1,2,0)  # (28,28,1)\n",
        "        img_resized = resize(img_np, (32,32,1), anti_aliasing=True).astype(np.float32)\n",
        "        imgs.append(img_resized)\n",
        "        labels.append(label)\n",
        "    imgs = np.array(imgs)\n",
        "    labels = np.array(labels)\n",
        "    return imgs, labels\n",
        "\n",
        "x_train_32, y_train = resize_dataset(train_dataset)\n",
        "x_test_32, y_test = resize_dataset(test_dataset)\n",
        "\n",
        "# Convert numpy arrays to torch tensors\n",
        "x_train_32 = torch.tensor(x_train_32).permute(0,3,1,2)  # (N,1,32,32)\n",
        "x_test_32 = torch.tensor(x_test_32).permute(0,3,1,2)\n",
        "y_train = torch.tensor(y_train).long()\n",
        "y_test = torch.tensor(y_test).long()\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(x_train_32, x_train_32), batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(x_test_32, x_test_32), batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7WLHdZ79Mzk"
      },
      "source": [
        "You can modify the code below to define your non-convolutional autoencoder. You can use any layer you want apart from `Conv2D` layers for this autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjyu1YvmlAQs"
      },
      "outputs": [],
      "source": [
        "### Non-convolutional Autoencoder ###\n",
        "class NonConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(32*32, 10),  # Representation with dimensionality 10\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(10, 32*32),\n",
        "            nn.Unflatten(1, (1, 32, 32)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        z = self.encoder(x)\n",
        "        x_recon = self.decoder(z)\n",
        "        return x_recon\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.flatten(x)\n",
        "        z = self.encoder(x)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5XsNpKrt2nS"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "autoencoder = NonConvAutoencoder()\n",
        "print(torchinfo.summary(autoencoder, input_size=(1, 1, 32, 32)))\n",
        "print()\n",
        "\n",
        "train(\n",
        "    autoencoder,\n",
        "    train_loader,\n",
        "    nn.MSELoss(),\n",
        "    optim.Adam(autoencoder.parameters()),\n",
        "    num_epochs = 20,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58upUwtD9S7x"
      },
      "source": [
        "You can modify the code below to define your convolutional autoencoder. For this autoencoder you need to include `Conv2D` layers in your design, but you can use any other layer too. We show an example of a simple convolutional architecture below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_s90fEyL8TB"
      },
      "outputs": [],
      "source": [
        "### Convolutional Autoencoder ###\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # output: 32x16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32*16*16, 10)  # representation 10 dim\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(10, 1*16*16),\n",
        "            nn.Unflatten(1, (1,16,16)),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),  # upsample to 32x32\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_recon = self.decoder(z)\n",
        "        return x_recon\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyJ_kM0vmYCa"
      },
      "outputs": [],
      "source": [
        "# You can modify the number of epochs or other hyperparameters\n",
        "set_seed(42)\n",
        "\n",
        "conv_autoencoder = ConvAutoencoder()\n",
        "print(torchinfo.summary(conv_autoencoder, input_size=(1, 1, 32, 32)))\n",
        "print()\n",
        "\n",
        "train(\n",
        "    conv_autoencoder,\n",
        "    train_loader,\n",
        "    nn.MSELoss(),\n",
        "    optim.Adam(conv_autoencoder.parameters()),\n",
        "    num_epochs = 20,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-bJ9xiNZzeA"
      },
      "source": [
        "Below you have the code you will use to train the classifier. We first extract the representations using any of the two autoencoders we just trained or PCA and then we train the classifier on top, which is just a simple Dense layer. Better representations should make it easier for the given simple classifier to separate the classes and, therefore, have larger accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dASdxeY9noR4"
      },
      "outputs": [],
      "source": [
        "### PCA ###\n",
        "pca = PCA(n_components=10)\n",
        "pca.fit(x_train_32.numpy().reshape(len(x_train_32), -1))\n",
        "\n",
        "## We compute the representations for the different methods\n",
        "representation_pca_train = pca.transform(x_train_32.numpy().reshape(len(x_train_32), -1))\n",
        "representation_pca_test = pca.transform(x_test_32.numpy().reshape(len(x_test_32), -1))\n",
        "\n",
        "# predict_representation is defined at the beginning of this notebook\n",
        "representation_auto_train = predict_representation(autoencoder, x_train_32)\n",
        "representation_auto_test = predict_representation(autoencoder, x_test_32)\n",
        "representation_conv_auto_train = predict_representation(conv_autoencoder, x_train_32)\n",
        "representation_conv_auto_test = predict_representation(conv_autoencoder, x_test_32)\n",
        "\n",
        "# Compute PCA reconstruction MSE\n",
        "reconst_train = pca.inverse_transform(representation_pca_train).reshape(-1,1,32,32)\n",
        "train_mse_pca = ((reconst_train - x_train_32.numpy())**2).mean()\n",
        "\n",
        "reconst_test = pca.inverse_transform(representation_pca_test).reshape(-1,1,32,32)\n",
        "test_mse_pca = ((reconst_test - x_test_32.numpy())**2).mean()\n",
        "\n",
        "# We print the MSE for PCA, which you need to include on the table\n",
        "print(f'PCA Train MSE: {train_mse_pca:.4f}')\n",
        "print(f'PCA Test MSE: {test_mse_pca:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAvclywongkQ"
      },
      "outputs": [],
      "source": [
        "### Linear classifier to test representation quality, DO NOT MODIFY IT!\n",
        "class LinearClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=10, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb3RzYrZ2hTZ"
      },
      "outputs": [],
      "source": [
        "def train_classifier(X_train, y_train, X_val, y_val, epochs=30):\n",
        "    set_seed(42)\n",
        "\n",
        "    X_train = torch.tensor(X_train).float()\n",
        "    X_val = torch.tensor(X_val).float()\n",
        "\n",
        "    train_ds = TensorDataset(X_train, y_train)\n",
        "    val_ds = TensorDataset(X_val, y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=128)\n",
        "\n",
        "    model = LinearClassifier(input_dim=X_train.shape[1], num_classes=10).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation accuracy\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                correct += (preds == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "        acc = correct / total\n",
        "\n",
        "        if (epoch+1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] Validation Accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "    return acc\n",
        "\n",
        "print(\"Training classifier on PCA representations\")\n",
        "acc_pca = train_classifier(representation_pca_train, y_train, representation_pca_test, y_test)\n",
        "print()\n",
        "\n",
        "print(\"Training classifier on Non-Convolutional Autoencoder representations\")\n",
        "acc_auto = train_classifier(representation_auto_train, y_train, representation_auto_test, y_test)\n",
        "print()\n",
        "\n",
        "print(\"Training classifier on Convolutional Autoencoder representations\")\n",
        "acc_conv_auto = train_classifier(representation_conv_auto_train, y_train, representation_conv_auto_test, y_test)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CogmYPqKAoqt"
      },
      "source": [
        "The code below can help you visualize the quality of the learnt representations. tSNE is a dimensionality reduction technique that leads to nice plots, so we reduce the representations of dimensionality 10 we just learnt to dimensionality 2 via tSNE and plot it. You do not have to include the figures in the report, it is just a qualitative way for you to see the quality of your representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUka6avw5Oh3"
      },
      "outputs": [],
      "source": [
        "## We use tSNE for our dimensionality reduction technique so we can\n",
        "## plot the features using a 2D plot as it leads to nice plots.\n",
        "## However, tSNE is tricky to use as a general dimensionality reduction method\n",
        "## for clustering due to issues mentioned here: https://distill.pub/2016/misread-tsne/\n",
        "## TSNE: https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\n",
        "## Nice article explaining shortcomings: https://distill.pub/2016/misread-tsne/\n",
        "\n",
        "## Use these parameters, the plots are highly dependent on perplexity value\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=40, max_iter=500, n_jobs=-1)\n",
        "representation_tsne = tsne.fit_transform(representation_auto_test)\n",
        "plot_representation_label(representation_tsne, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqqx1Q1WMCRY"
      },
      "source": [
        "You can also check how the reconstructed images look with the autoencoders you just trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEcnL2fMMBtP"
      },
      "outputs": [],
      "source": [
        "ind = np.random.randint(x_test.shape[0] -  1)\n",
        "## The function below is defined in the tutorial\n",
        "plot_recons_original(np.expand_dims(x_test_32[ind],0), y_test[ind], conv_autoencoder, size_image=(32,32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJeMc4vKo7He"
      },
      "source": [
        "## Task 2: Custom Loss Functions\n",
        "\n",
        "In Image-to-Image tasks, researchers have found that an approach to improve the robustness of autoencoders is to replace the quadratic error with a loss function that is more robust to outliers.\n",
        "\n",
        "There is a lot of interest in defining which loss function helps the most specific tasks. For instance, super-image resolution and image denoising may have different optimum loss functions, even though, they are trained in a very similar manner. Therefore, in this task, we will focus on using multiple loss functions to train an image denoise model. Sometimes, you may need to use a loss function that is not defined in Keras. If that happens, you can define it yourself and use it in the model.compile() module. We explain now how to do that.\n",
        "\n",
        "You must define some variables:\n",
        "\n",
        "* **True values** are those that we are aiming to generate, e.g. GT images.\n",
        "* **Predicted values** are those that the network has generated,  e.g. denoised images.\n",
        "* **Loss value** is the computed loss between true and predicted values,  e.g. MSE value.\n",
        "\n",
        "The common structure for the custom loss method is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4XZmYuhAQE9"
      },
      "outputs": [],
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, predicted, target):\n",
        "        return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3XEZlhqAQFf"
      },
      "source": [
        "We are going to use TinyImagenet, and add synthetic noise as before. This task requires a lot of RAM, thus, before starting it, please clean your RAM memory by restarting the Colab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWEjAN7Kp_TY"
      },
      "outputs": [],
      "source": [
        "# download TinyImageNet\n",
        "! git clone https://github.com/seshuad/IMagenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzxuuXj_AQFf"
      },
      "outputs": [],
      "source": [
        "class TinyImageNetNoisyDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None, noise_std=0.2):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.noise_std = noise_std\n",
        "        self.image_files = []\n",
        "\n",
        "        if self.split == 'train':\n",
        "            # For training data, images are in subfolders per class\n",
        "            wnids_path = os.path.join(root_dir, 'tiny-imagenet-200', 'wnids.txt')\n",
        "            with open(wnids_path, 'r') as f:\n",
        "                classes = [line.strip() for line in f]\n",
        "            for cls in classes:\n",
        "                image_dir = os.path.join(root_dir, 'tiny-imagenet-200', split, cls, 'images')\n",
        "                for img_file in os.listdir(image_dir):\n",
        "                    if img_file.endswith('.JPEG'):\n",
        "                        self.image_files.append(os.path.join(image_dir, img_file))\n",
        "\n",
        "        elif self.split == 'val':\n",
        "            # For validation data, images are in a single folder and annotations are in a file\n",
        "            annotations_path = os.path.join(root_dir, 'tiny-imagenet-200', split, 'val_annotations.txt')\n",
        "            with open(annotations_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    img_name, _, _, _, _, _ = line.strip().split('\\t')\n",
        "                    self.image_files.append(os.path.join(root_dir, 'tiny-imagenet-200', split, 'images', img_name))\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid split: {split}. Use 'train' or 'val'.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        noisy_img = img + torch.randn_like(img) * self.noise_std\n",
        "        noisy_img = torch.clamp(noisy_img, 0, 1)\n",
        "\n",
        "        return noisy_img, img  # return noisy input and clean target\n",
        "\n",
        "# Image transformations (resizing and normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Corrected root_dir to point to the base directory of TinyImageNet\n",
        "train_dataset = TinyImageNetNoisyDataset('./IMagenet', transform=transform)\n",
        "test_dataset = TinyImageNetNoisyDataset('./IMagenet', split='val', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GKRQn41JpXh"
      },
      "source": [
        "We are going to use the UNet architecture for this task, however, you could use any autoencoder of your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdvzRDqvJeqO"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(512, 1024, 3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Decoder\n",
        "        self.up6 = nn.Conv2d(1024, 512, 3, padding=1)\n",
        "        self.up7 = nn.Conv2d(512, 256, 3, padding=1)\n",
        "        self.up8 = nn.Conv2d(256, 128, 3, padding=1)\n",
        "        self.up9 = nn.Conv2d(128, 64, 3, padding=1)\n",
        "\n",
        "        self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.conv_final = nn.Conv2d(64, 3, 3, padding=1)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        c1 = self.relu(self.conv1(x))\n",
        "        p1 = self.pool(c1)\n",
        "\n",
        "        c2 = self.relu(self.conv2(p1))\n",
        "        p2 = self.pool(c2)\n",
        "\n",
        "        c3 = self.relu(self.conv3(p2))\n",
        "        p3 = self.pool(c3)\n",
        "\n",
        "        c4 = self.relu(self.conv4(p3))\n",
        "        d4 = self.dropout(c4)\n",
        "        p4 = self.pool(d4)\n",
        "\n",
        "        c5 = self.relu(self.conv5(p4))\n",
        "        d5 = self.dropout(c5)\n",
        "\n",
        "        # Decoder\n",
        "        up6 = self.up_sample(d5)\n",
        "        up6 = self.relu(self.up6(up6))\n",
        "        merge6 = torch.cat([d4, up6], dim=1)\n",
        "        c6 = self.relu(self.up6(merge6))\n",
        "\n",
        "        up7 = self.up_sample(c6)\n",
        "        up7 = self.relu(self.up7(up7))\n",
        "        merge7 = torch.cat([c3, up7], dim=1)\n",
        "        c7 = self.relu(self.up7(merge7))\n",
        "\n",
        "        up8 = self.up_sample(c7)\n",
        "        up8 = self.relu(self.up8(up8))\n",
        "        merge8 = torch.cat([c2, up8], dim=1)\n",
        "        c8 = self.relu(self.up8(merge8))\n",
        "\n",
        "        up9 = self.up_sample(c8)\n",
        "        up9 = self.relu(self.up9(up9))\n",
        "        merge9 = torch.cat([c1, up9], dim=1)\n",
        "        c9 = self.relu(self.up9(merge9))\n",
        "\n",
        "        output = self.conv_final(c9)\n",
        "        output = torch.sigmoid(output)  # constrain output to [0,1]\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKwr7T1nJG2S"
      },
      "source": [
        "Now, an example of how to train UNet architecture with a custom MSE loss function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pgxz_-weyy1"
      },
      "outputs": [],
      "source": [
        "class CustomMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, predicted, target):\n",
        "        return torch.mean((predicted - target) ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtP8WmAWe74_"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "model = UNet()\n",
        "print(torchinfo.summary(model, input_size=(1, 3, 64, 64)))\n",
        "print()\n",
        "\n",
        "train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    CustomMSELoss(),\n",
        "    optim.Adam(model.parameters(), lr=1e-4),\n",
        "    num_epochs = 10,\n",
        ")\n",
        "print()\n",
        "\n",
        "evaluate(model, test_loader, CustomMSELoss())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCptRFISDnrL"
      },
      "source": [
        "Use the following code for visualising examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVe3YrKtDmn1"
      },
      "outputs": [],
      "source": [
        "def show_noisy_denoised_clean(idx=0):\n",
        "    model.eval()\n",
        "    noisy_img, clean_img = test_dataset[idx]\n",
        "    noisy_img_t = noisy_img.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        denoised_img = model(noisy_img_t).cpu().squeeze(0)\n",
        "\n",
        "    # Convert tensors to numpy arrays for plotting\n",
        "    noisy_np = noisy_img.permute(1, 2, 0).numpy()\n",
        "    denoised_np = denoised_img.permute(1, 2, 0).numpy()\n",
        "    clean_np = clean_img.permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Plot side by side\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(12,4))\n",
        "    axs[0].imshow(np.clip(noisy_np, 0, 1))\n",
        "    axs[0].set_title('Noisy Input')\n",
        "    axs[0].axis('off')\n",
        "    axs[1].imshow(np.clip(denoised_np, 0, 1))\n",
        "    axs[1].set_title('Denoised Output')\n",
        "    axs[1].axis('off')\n",
        "    axs[2].imshow(np.clip(clean_np, 0, 1))\n",
        "    axs[2].set_title('Clean Target')\n",
        "    axs[2].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "show_noisy_denoised_clean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChyCg8V9AQFi"
      },
      "source": [
        "Try to use a different loss function and see which one gives you the best result. Some well-known loss functions for image denoising are:\n",
        "\n",
        "*  Structural Similarity Index ([SSIM](https://en.wikipedia.org/wiki/Structural_similarity))\n",
        "*  Multiscale Structural Similarity Index ([MS-SSIM](https://ieeexplore.ieee.org/document/1292216?arnumber=1292216&tag=1))\n",
        "* 1 / [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio)\n",
        "* MAE\n",
        "* [L0](https://arxiv.org/pdf/1803.04189.pdf)\n",
        "\n",
        "Check the [Noise2Noise](https://arxiv.org/pdf/1803.04189.pdf) paper to learn more about alternative losses.\n",
        "\n",
        "**Report:**\n",
        "*  In this task, you are asked to build a table containing the MSE results on the test split of models trained with different loss functions. Use two or three different loss functions from the previous list and discuss the differences you observe. Report denoised images to support your arguments. You may need to modify the UNet model definition to use some of the previous losses.\n",
        "\n",
        "Use the following code and write your custom loss function within the provided `CustomLoss` class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6v_-PShE-Jn"
      },
      "outputs": [],
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, predicted, target):\n",
        "        '''\n",
        "        Define your loss here\n",
        "        '''\n",
        "        return 0"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
