{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/02_CNN_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0ZorPBtpLlZ"
      },
      "source": [
        "# Coursework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gkM3dPZbNcX"
      },
      "source": [
        "## Task 1: Classification\n",
        "\n",
        "At this point, we know what is a CNN, how they work, and the components needed to design them. In this first task, we want you to create a CNN that is able to outperform the Multi-layer Perceptron model from Tutorial 1. For the first part of the coursework, we train on CIFAR10, a  classical dataset for image classification. Note that in these tutorials, we mainly use the official test sets of several standard datasets as our validation data. The reason we use the given test sets as validation data for the tutorials is that is an easy way to make sure that we all work with the same split and report results using the same data. However, in a proper machine learning setup, your validation set should be separate from the test set, so you can tune the model/parameters on the validation set and then check the final performance in the test set. Thus, even though the variables are `x_test` and `y_test`, they represent our validation set.\n",
        "\n",
        "Let's first load the dataset and visualise some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANF4Gf5LY8Nv"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Visualize some examples\n",
        "X_train = train_dataset.data       # NumPy array of shape (50000, 32, 32, 3)\n",
        "y_train = np.array(train_dataset.targets)\n",
        "\n",
        "X_test = test_dataset.data\n",
        "y_test = np.array(test_dataset.targets)\n",
        "\n",
        "print('Image shape: {0}'.format(X_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(X_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(X_test.shape[0]))\n",
        "\n",
        "N = 5\n",
        "start_val = 0  # pick an element for the code to plot the following N**2 values\n",
        "fig, axes = plt.subplots(N, N, figsize=(8, 8))\n",
        "class_names = train_dataset.classes  # List of class names: ['airplane', 'automobile', ..., 'truck']\n",
        "\n",
        "for row in range(N):\n",
        "    for col in range(N):\n",
        "        idx = start_val + row + N * col\n",
        "        axes[row, col].imshow(X_train[idx])\n",
        "        label_idx = y_train[idx]\n",
        "        axes[row, col].set_title(class_names[label_idx])\n",
        "        axes[row, col].set_xticks([])\n",
        "        axes[row, col].set_yticks([])\n",
        "\n",
        "fig.subplots_adjust(hspace=0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxFx2s3Fc6wM"
      },
      "source": [
        "Now, we are ready to define the Multi-layer Perceptron model and train it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1J5pMG9Zu3Q"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Define the model\n",
        "class FullyConnectedNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(3072, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Instantiate model, loss function, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = FullyConnectedNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=1e-4)\n",
        "epochs = 20\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = inputs.view(inputs.size(0), -1) # Flatten the input for MLP\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == targets).sum().item()\n",
        "        total += inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc*100:.2f}%\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "loss_total = 0.0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        xb = xb.view(xb.size(0), -1)\n",
        "        preds = model(xb)\n",
        "        loss_total += criterion(preds, yb).item() * xb.size(0)\n",
        "        predicted = torch.argmax(preds, dim=1)\n",
        "        correct += (predicted == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "print()\n",
        "print(f\"Validation loss: {loss_total/total:.4f}, Validation accuracy: {correct/total*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEuYNmyHlVJd"
      },
      "source": [
        "### Problem Definition\n",
        "\n",
        "In this exercise, you are asked to test several CNN architectures in the code provided below. Do not modify the optimizer, loss used or parameters related to the training such as the learning rate, they will be investigated in future tutorials. You must focus on the architecture itself: number of convolutional layers, number of filters in every layer, activation functions, pooling operators, among others. Batch Normalization and Dropout layers, which are quite used in CNN architectures, will be also investigated in a future tutorial so you do not have to discuss them.\n",
        "\n",
        "\n",
        "**Report**:\n",
        "*   Present a bar figure with the training and validation accuracies for different design choices. Discuss only the parameters that have a significant influence on the network's performance. Explain any discrepancy between training and validation accuracies.\n",
        "*   Present a sketch that introduces your best architecture. See some examples on how to display networks in [cv-tricks' blog](https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9_KYEY5cgH1"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "# Here we are using the official test set as our validation set, in further\n",
        "# tutorials, test and validation splits will be explained properly.\n",
        "# Hence, even though the variables are `x_test` and `y_test`, they represent our validation set\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# TODO: Define your architecture here\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=1e-4)\n",
        "epochs = 20\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == targets).sum().item()\n",
        "        total += inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc*100:.2f}%\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for input, target in test_loader:\n",
        "        input, target = input.to(device), target.to(device)\n",
        "        output = model(input)\n",
        "        test_loss += criterion(output, target).item() * input.size(0)\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += (pred == target).sum().item()\n",
        "\n",
        "test_loss /= len(test_dataset)\n",
        "accuracy = 100. * correct / len(test_dataset)\n",
        "\n",
        "print()\n",
        "print(f'Validation loss: {test_loss:.4f}, Validation accuracy: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w8eUV9RDPkL"
      },
      "source": [
        "\n",
        "---\n",
        "---\n",
        "\n",
        "## Task 2: Regression\n",
        "\n",
        "Now we face a regression task instead of a classification problem. Loss function, activations, and dataset will change in the following task. Thus, instead of having one vector with the probabilities of each class, in this regression problem, the output is a single scalar.\n",
        "\n",
        "For this second task, we chose the task of estimating house prices based on input images. To get the data run the following script, which clones Ahmed and Moustafa’s [repository](https://github.com/emanhamed/Houses-dataset) into colmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibukle2ODMEp"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/emanhamed/Houses-dataset\n",
        "%cd /content/Houses-dataset/Houses\\ Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye6xldkVEV-t"
      },
      "source": [
        "This dataset contains four images of the house (kitchen, frontal, bedroom and bathroom), and attributes (number of bedrooms, number of bathrooms, zip code...). For our exercise, we only use the images of the house. We start with front door images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71urud9ZHq9d"
      },
      "outputs": [],
      "source": [
        "house_section = 'frontal' # select between: kitchen, frontal, bedroom or bathroom\n",
        "print('We have selected {} images. You can switch to the kitchen, frontal, bedroom or bathroom images by changing house_section variable.'.format(house_section))\n",
        "images = []\n",
        "for i_im in range(1, 536):\n",
        "  image = cv2.imread(str(i_im)+'_'+house_section+'.jpg')\n",
        "  image = cv2.resize(image, (64, 64))\n",
        "  images.append(image)\n",
        "\n",
        "labels = []\n",
        "f = open('HousesInfo.txt', \"r\")\n",
        "for x in f:\n",
        "  label = (x).split(' ')[-1].split('\\n')[0]\n",
        "  labels.append(label)\n",
        "\n",
        "# Let's visualize some examples\n",
        "N=3\n",
        "start_val = 0 # pick an element for the code to plot the following N**2 values\n",
        "fig, axes = plt.subplots(N,N)\n",
        "for row in range(N):\n",
        "  for col in range(N):\n",
        "    idx = start_val+row+N*col\n",
        "    tmp = cv2.cvtColor(images[idx],cv2.COLOR_BGR2RGB)\n",
        "    axes[row,col].imshow(tmp, cmap='gray')\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    target = int(labels[idx])\n",
        "    axes[row,col].set_title(str(target) + '$')\n",
        "    axes[row,col].set_xticks([])\n",
        "    axes[row,col].set_yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scNfczb73nGS"
      },
      "source": [
        "Prepare the dataset for training the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FORNJ1QU2L-t"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Convert to NumPy arrays and normalize\n",
        "images = np.asarray(images).astype(np.float32) / 255.0  # Normalize pixel values\n",
        "labels = np.asarray(labels).astype(np.float32)\n",
        "\n",
        "# Normalize labels\n",
        "max_price = labels.max()\n",
        "labels /= max_price\n",
        "\n",
        "# Shuffle\n",
        "indices = np.random.permutation(len(images))\n",
        "images = images[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# Split into train and validation\n",
        "split_idx = int(0.8 * len(images))\n",
        "X_train_np, X_val_np = images[:split_idx], images[split_idx:]\n",
        "Y_train_np, Y_val_np = labels[:split_idx], labels[split_idx:]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "# If images are in (N, H, W, C) format (NHWC), convert to (N, C, H, W)\n",
        "X_train = torch.from_numpy(X_train_np).permute(0, 3, 1, 2)  # NHWC → NCHW\n",
        "X_val = torch.from_numpy(X_val_np).permute(0, 3, 1, 2)\n",
        "Y_train = torch.from_numpy(Y_train_np)\n",
        "Y_val = torch.from_numpy(Y_val_np)\n",
        "\n",
        "# Create TensorDataset and DataLoader\n",
        "batch_size = 32\n",
        "train_loader = data.DataLoader(data.TensorDataset(X_train, Y_train), batch_size=batch_size, shuffle=True)\n",
        "val_loader = data.DataLoader(data.TensorDataset(X_val, Y_val), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Check shape\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('Y_train shape:', Y_train.shape)\n",
        "print('X_val shape:', X_val.shape)\n",
        "print('Y_val shape:', Y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGC6dXqdM2eU"
      },
      "source": [
        "### Problem Definition\n",
        "\n",
        "Similar to the previous task, you are asked to design a CNN architecture able to perform the estimation of house prices based on the `frontal` house image. Design a new model by changing parameters such as the number of convolutional layers, activation functions, strides, or pooling operators, among others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBbilMqykwuK"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# MAPE\n",
        "def mean_absolute_percentage_error(y_pred, y_true):\n",
        "    return torch.mean(torch.abs((y_true - y_pred) / (y_true + 1e-12))) * 100\n",
        "\n",
        "# TODO: Define here your architecture\n",
        "class HousePriceModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define your layers here\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define your forward pass here\n",
        "        raise NotImplementedError()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = HousePriceModel().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_sample = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = mean_absolute_percentage_error(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        train_sample += inputs.size(0)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_sample = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss += mean_absolute_percentage_error(outputs.squeeze(), targets).item() * inputs.size(0)\n",
        "            val_sample += inputs.size(0)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/100] - Train loss: {train_loss/train_sample:.2f}%, Validation loss: {val_loss/val_sample:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyXrXBFVQ-tL"
      },
      "source": [
        "The metric used in this problem to evaluate the performance is the same we used for training the model, the mean absolute percentage error. Mean absolute percentage error is defined as $\\frac{100}{n} \\sum_n \\frac{|\\hat{y} - y|}{|y|}$ where $y$ is the ground-truth, $\\hat{y}$ is the estimation of the model and `n` the number of elements in the set we are evaluating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYpLWzJOfJyr"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "error_total = 0.0\n",
        "sample_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in val_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        preds = model(xb)\n",
        "        error_total += mean_absolute_percentage_error(preds.squeeze(), yb).item() * xb.size(0)\n",
        "        sample_total += xb.size(0)\n",
        "\n",
        "print(f\"Predicting house prices - Estimation Error: {error_total/sample_total:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2uJs6-xG6lD"
      },
      "source": [
        "**Report**:\n",
        "\n",
        "\n",
        "*   Propose a CNN architecture that has an estimation error in the validation set below 75%.\n",
        "*   Present a figure showing the training and validation loss vs the number of training epochs for different architectural design choices. Discuss the gap between the training and validation loss depending on the proposed architecture.\n",
        "*   Report a table with results when using any of the other images from the house (kitchen, bedroom, and bathroom)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "generative_ai_disabled": true,
      "gpuType": "T4",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
